{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p/pmkim/strokach/anaconda3/lib/python3.6/site-packages/kmbio/PDB/io/__init__.py:9: UserWarning: Could not import viewers!\n",
      "  warnings.warn(\"Could not import viewers!\")\n",
      "Setting the PACKAGE_VERSION environment variable.\n",
      "Setting the DOCS_SECRET_KEY environment variable.\n",
      "Setting the PYTHON_VERSION environment variable.\n",
      "Setting the SPARK_MASTER environment variable.\n",
      "Setting the SPARK_ARGS environment variable.\n",
      "Setting the DB_TYPE environment variable.\n",
      "Setting the DB_PORT environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-30 13:41:37.891125\n"
     ]
    }
   ],
   "source": [
    "%run _imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pagnn\n",
    "import pagnn.training.gan\n",
    "import pagnn.prediction.gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagnn.settings.CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = 'validate_trained_network'\n",
    "NOTEBOOK_PATH = Path(NOTEBOOK_NAME)\n",
    "NOTEBOOK_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_files = {\n",
    "    'permute': 'validation_gan_permute_80_1000.pickle',\n",
    "    'exact': 'validation_gan_exact_80_1000.pickle',\n",
    "    'start': 'validation_gan_start_80_1000.pickle',\n",
    "    'stop': 'validation_gan_stop_80_1000.pickle',\n",
    "    'middle': 'validation_gan_middle_80_1000.pickle',\n",
    "    'edges': 'validation_gan_edges_80_1000.pickle',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_data = dict()\n",
    "for name, file in validation_files.items():\n",
    "    with Path('train_neural_network').joinpath(file).open('rb') as fin:\n",
    "        validation_data[name] = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataSetGAN(seqs=[b'YQLEKDPIAGAETFYVDGAANRETKLGKAGYVTDRGRRKIVSLTETTNQKTELQAIYIALQDSGSEVNIVTDSQYALGIIQAQPDKSESELVNQIIEQLIGKERVYLSWVPAHKGIGGNEQVDKLVSSG', b'VTDRGRRKIVSLTETTNQKTELQAIYIALQDSGSEVNIVTDSQYALGIIQAQPDKSESELVNQIIEQLIGKERVYLSWVPAHKGIGGNEQVDKLVSSGYQLEKDPIAGAETFYVDGAANRETKLGKAGY'], adjs=[<129x129 sparse matrix of type '<class 'numpy.int16'>'\n",
       "\twith 1682 stored elements in COOrdinate format>], targets=[1, 0], meta=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data['permute'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutation_data = dict()\n",
    "for name in ['protherm', 'humsavar']:\n",
    "    mutation_data[name] = pagnn.training.gan.get_mutation_dataset(name, NOTEBOOK_PATH.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def load_networks(unique_name, step):\n",
    "\n",
    "    args = pagnn.prediction.gan.Args(\n",
    "        input_file='',\n",
    "        output_file='',\n",
    "        work_path=(\n",
    "            NOTEBOOK_PATH.parent\n",
    "            .joinpath('train_neural_network')\n",
    "            .joinpath(unique_name)),\n",
    "        step=step,\n",
    "        nseqs=10_000,\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    args_training = pagnn.training.gan.Args(root_path=args.work_path.parent)\n",
    "    args_training.unique_name = args.work_path.name\n",
    "\n",
    "    # Load network\n",
    "    net_d = pagnn.models.AESeqAdjApplyExtra(\n",
    "        'discriminator',\n",
    "        hidden_size=args_training.hidden_size,\n",
    "        bottleneck_size=1,\n",
    "    )\n",
    "\n",
    "    net_g = pagnn.models.AESeqAdjApplyExtra(\n",
    "        'generator',\n",
    "        hidden_size=args_training.hidden_size,\n",
    "        bottleneck_size=16,\n",
    "        encoder_network=net_d,\n",
    "    )\n",
    "\n",
    "    net_d.load_state_dict(\n",
    "        torch.load(\n",
    "            args_training.work_path.joinpath('models').joinpath(f'net_d-step_{args.step}.model')\n",
    "            .as_posix(),\n",
    "            map_location='cpu'))\n",
    "    net_g.load_state_dict(\n",
    "        torch.load(\n",
    "            args_training.work_path.joinpath('models').joinpath(f'net_g-step_{args.step}.model')\n",
    "            .as_posix(),\n",
    "            map_location='cpu'))\n",
    "    \n",
    "    return net_d, net_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_bottleneck_indices(adjs):\n",
    "    idxs = []\n",
    "    start = 0\n",
    "    for i, adj in enumerate(adjs):\n",
    "        stop = start + adj[4].shape[1]\n",
    "        idxs.append((math.floor(start / 4), math.ceil(stop / 4),))\n",
    "        start = stop\n",
    "    assert idxs[-1][1] == math.ceil(sum(adj[4].shape[1] for adj in adjs) / 4)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_validation_dataset(net_d, datasets, batch_size):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        A tuple of targets and outputs arrays.\n",
    "            - Targets are ΔΔG values.\n",
    "            - Outputs are (pred_mut [low] - pred_wt [high]), so they should be *positive* for\n",
    "              stabilizing mutations and *negative* for destabilizing mutations (i.e. the\n",
    "              *reverse* of ΔΔG).\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    datasets = iter(datasets)\n",
    "    \n",
    "    last_run = False\n",
    "    while not last_run:\n",
    "        batch_pos = ([], [], [])\n",
    "        batch_neg = ([], [], [])\n",
    "        while len(batch_pos[0]) < batch_size:\n",
    "            try:\n",
    "                dataset = next(datasets)\n",
    "            except StopIteration:\n",
    "                last_run = True\n",
    "                break\n",
    "            else:\n",
    "                datavar = net_d.dataset_to_datavar(dataset)\n",
    "                batch_pos[0].append(datavar.seqs[0:1, :, :])\n",
    "                batch_pos[1].append(datavar.adjs)\n",
    "                batch_pos[2].append(dataset.targets[0])  # 1 / 1 / 1\n",
    "                batch_neg[0].append(datavar.seqs[1:2, :, :])\n",
    "                batch_neg[1].append(datavar.adjs)\n",
    "                batch_neg[2].append(dataset.targets[1])  # 0 / ddG / 0 or 1\n",
    "\n",
    "        if len(batch_pos[0]) == 0:\n",
    "            break\n",
    "\n",
    "        datavar_pos = (\n",
    "            torch.cat(batch_pos[0], 2),\n",
    "            batch_pos[1],\n",
    "        )\n",
    "        datavar_neg = (\n",
    "            torch.cat(batch_neg[0], 2),\n",
    "            batch_neg[1],\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_pos = net_d(*datavar_pos)\n",
    "            output_neg = net_d(*datavar_neg)\n",
    "\n",
    "        output = []\n",
    "        target = []\n",
    "        for i, (start, stop) in enumerate(get_bottleneck_indices(datavar_pos[1])):\n",
    "            output.append(float(output_pos[:, :, start:stop].sigmoid().mean()))\n",
    "            output.append(float(output_neg[:, :, start:stop].sigmoid().mean()))\n",
    "            target.extend([batch_pos[2][i], batch_neg[2][i]])\n",
    "        assert (i + 1) == len(batch_pos[2]) == len(batch_neg[2])\n",
    "        assert output_pos.shape == output_neg.shape\n",
    "        assert stop <= output_pos.shape[2] <= (stop + 1)\n",
    "        \n",
    "        outputs.extend(output)\n",
    "        targets.extend(target)\n",
    "\n",
    "    outputs_ar = np.array(outputs)\n",
    "    targets_ar = np.array(targets)\n",
    "    return targets_ar, outputs_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_mutation_dataset(net_d, datasets, batch_size):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "    A tuple of targets and outputs arrays.\n",
    "        - Targets are 0 for benign, -1 for deleterious.\n",
    "        - Outputs are (pred_mut [low] - pred_wt [high]), so they should be *positive* for\n",
    "            stabilizing mutations and *negative* for destabilizing mutations (i.e. the\n",
    "            *reverse* of ΔΔG).\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    datasets = iter(datasets)\n",
    "    \n",
    "    last_run = False\n",
    "    while not last_run:\n",
    "        batch_pos = ([], [], [])\n",
    "        while len(batch_pos[0]) < batch_size:\n",
    "            try:\n",
    "                dataset = next(datasets)\n",
    "            except StopIteration:\n",
    "                last_run = True\n",
    "                break\n",
    "            else:\n",
    "                datavar = net_d.dataset_to_datavar(dataset)\n",
    "                # Pos\n",
    "                batch_pos[0].append(datavar.seqs[0:1, :, :])\n",
    "                batch_pos[1].append(datavar.adjs)\n",
    "                batch_pos[2].append(dataset.targets[0])  # 1 / 1 / 1\n",
    "                # Neg\n",
    "                batch_pos[0].append(datavar.seqs[1:2, :, :])\n",
    "                batch_pos[1].append(datavar.adjs)\n",
    "                batch_pos[2].append(dataset.targets[1])  # 0 / ddG / 0 or 1\n",
    "\n",
    "        if len(batch_pos[0]) == 0:\n",
    "            break\n",
    "\n",
    "        datavar_pos = (\n",
    "            torch.cat(batch_pos[0], 2),\n",
    "            batch_pos[1],\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_pos = net_d(*datavar_pos)\n",
    "\n",
    "        output = []\n",
    "        target = []\n",
    "        for i, (start, stop) in enumerate(get_bottleneck_indices(datavar_pos[1])):\n",
    "            output.append(float(output_pos[:, :, start:stop].sigmoid().mean()))\n",
    "            target.append(batch_pos[2][i])\n",
    "        assert (i + 1) == len(batch_pos[2])\n",
    "        assert stop <= output_pos.shape[2] <= (stop + 1)\n",
    "        \n",
    "        outputs.extend(output)\n",
    "        targets.extend(target)\n",
    "\n",
    "    outputs_ar = np.array(outputs)\n",
    "    targets_ar = np.array(targets)\n",
    "    return targets_ar, outputs_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def worker(unique_name, step=12462):\n",
    "    step = int(step)\n",
    "    net_d, net_g = load_networks(unique_name, step)\n",
    "\n",
    "    net_d.train()\n",
    "    net_g.train()\n",
    "\n",
    "    assert net_d.training\n",
    "    assert net_g.training\n",
    "\n",
    "    all_scores = {}\n",
    "    for fn in [evaluate_mutation_dataset, evaluate_validation_dataset]:\n",
    "        for batch_size in [1, 64]:\n",
    "            scores = {}\n",
    "            for name, datasets in validation_data.items():\n",
    "                targets_valid, outputs_valid = fn(net_d, datasets, batch_size)\n",
    "                scores.update({\n",
    "                    f'{name}-auc': metrics.roc_auc_score(targets_valid, outputs_valid),\n",
    "                    f'{name}-targets-mean': targets_valid.mean(),\n",
    "                    f'{name}-outputs-mean': outputs_valid.mean(),\n",
    "                })\n",
    "            for name, datasets in mutation_data.items():\n",
    "                targets_valid, outputs_valid = fn(net_d, datasets, batch_size)\n",
    "                targets_muts = targets_valid[1::2]\n",
    "                outputs_muts = outputs_valid[1::2] - outputs_valid[0::2]\n",
    "                if 'protherm' in name:\n",
    "                    # Protherm predicts ΔΔG, so positive values are destabilizing\n",
    "                    scores[f'{name}-spearman_corr'] = sp.stats.spearmanr(-targets_valid, outputs_valid).correlation\n",
    "                elif 'humsavar' in name:\n",
    "                    # For humsavar: 0 = stable, 1 = deleterious\n",
    "                    scores[f'{name}-auc'] = metrics.roc_auc_score(1 - targets_valid, outputs_valid)\n",
    "                else:\n",
    "                    scores[f'{name}-auc'] = metrics.roc_auc_score(targets_valid + 1, outputs_valid)\n",
    "                scores.update({\n",
    "                    f'{name}-targets-mean': targets_valid.mean(),\n",
    "                    f'{name}-outputs-mean': outputs_valid.mean(),\n",
    "                })\n",
    "            all_scores[(fn.__name__, batch_size)] = scores\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_name = 'permute-seq-0-test_x14-0.1.9.dev-4a07eef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = (\n",
    "    NOTEBOOK_PATH\n",
    "    .parent\n",
    "    .joinpath('train_neural_network')\n",
    "    .joinpath(unique_name)\n",
    "    .joinpath('models')\n",
    ")\n",
    "d_models = [int(re.findall('net_d-step_(\\d+).model', str(p))[0]) for p in model_path.glob('net_d-step_*.model')]\n",
    "g_models = [int(re.findall('net_g-step_(\\d+).model', str(p))[0]) for p in model_path.glob('net_g-step_*.model')]\n",
    "assert not set(d_models) ^ set(g_models)\n",
    "steps = d_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'step': d_models}, index=range(len(d_models)))\n",
    "df['unique_name'] = 'permute-seq-0-test_x14-0.1.9.dev-4a07eef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task_id = int(os.environ['SLURM_ARRAY_TASK_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[task_id:task_id + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scores = [\n",
    "    worker(unique_name, step)\n",
    "    for unique_name, step\n",
    "    in zip(df['unique_name'].values, df['step'].values)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with NOTEBOOK_PATH.joinpath(f'all_scores_{task_id}.pickle').open('wb') as fout:\n",
    "    pickle.dump(all_scores, fout, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Done!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2300d1589a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Done!"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_trained_network/all_scores.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls {NOTEBOOK_PATH}/all_scores.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with NOTEBOOK_PATH.joinpath('all_scores_0.pickle').open('rb') as fin:\n",
    "    data = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('evaluate_mutation_dataset', 1): {'edges-auc': 0.599506,\n",
       "   'edges-outputs-mean': 0.6557192423933427,\n",
       "   'edges-targets-mean': 0.5,\n",
       "   'exact-auc': 0.617356,\n",
       "   'exact-outputs-mean': 0.6807881731571156,\n",
       "   'exact-targets-mean': 0.5,\n",
       "   'humsavar-auc': 0.5327501649326212,\n",
       "   'humsavar-outputs-mean': 0.7160360887895476,\n",
       "   'humsavar-targets-mean': 0.7586159360352909,\n",
       "   'middle-auc': 0.6135455,\n",
       "   'middle-outputs-mean': 0.6350259415384935,\n",
       "   'middle-targets-mean': 0.5,\n",
       "   'permute-auc': 0.6127520000000001,\n",
       "   'permute-outputs-mean': 0.6178972894671771,\n",
       "   'permute-targets-mean': 0.5,\n",
       "   'protherm-outputs-mean': 0.753119285922337,\n",
       "   'protherm-spearman_corr': -0.04370535501024037,\n",
       "   'protherm-targets-mean': 1.0729990058253231,\n",
       "   'start-auc': 0.6127005000000001,\n",
       "   'start-outputs-mean': 0.6341591125544404,\n",
       "   'start-targets-mean': 0.5,\n",
       "   'stop-auc': 0.6058479999999999,\n",
       "   'stop-outputs-mean': 0.6565360083819569,\n",
       "   'stop-targets-mean': 0.5},\n",
       "  ('evaluate_mutation_dataset', 64): {'edges-auc': 0.506299,\n",
       "   'edges-outputs-mean': 0.3745823943208179,\n",
       "   'edges-targets-mean': 0.5,\n",
       "   'exact-auc': 0.506231,\n",
       "   'exact-outputs-mean': 0.5007991187209263,\n",
       "   'exact-targets-mean': 0.5,\n",
       "   'humsavar-auc': 0.5086371558092035,\n",
       "   'humsavar-outputs-mean': 0.765887853341998,\n",
       "   'humsavar-targets-mean': 0.7586159360352909,\n",
       "   'middle-auc': 0.505155,\n",
       "   'middle-outputs-mean': 0.33806184383520305,\n",
       "   'middle-targets-mean': 0.5,\n",
       "   'permute-auc': 0.50871,\n",
       "   'permute-outputs-mean': 0.35253234976378106,\n",
       "   'permute-targets-mean': 0.5,\n",
       "   'protherm-outputs-mean': 0.9473861333179185,\n",
       "   'protherm-spearman_corr': -0.017556129884547644,\n",
       "   'protherm-targets-mean': 1.0729990058253231,\n",
       "   'start-auc': 0.49161599999999994,\n",
       "   'start-outputs-mean': 0.36262800938676265,\n",
       "   'start-targets-mean': 0.5,\n",
       "   'stop-auc': 0.5074909999999999,\n",
       "   'stop-outputs-mean': 0.4411435540059574,\n",
       "   'stop-targets-mean': 0.5},\n",
       "  ('evaluate_validation_dataset', 1): {'edges-auc': 0.6468185,\n",
       "   'edges-outputs-mean': 0.7141222543971438,\n",
       "   'edges-targets-mean': 0.5,\n",
       "   'exact-auc': 0.6339835,\n",
       "   'exact-outputs-mean': 0.7430965868359419,\n",
       "   'exact-targets-mean': 0.5,\n",
       "   'humsavar-auc': 0.5492138497115897,\n",
       "   'humsavar-outputs-mean': 0.7102713445205024,\n",
       "   'humsavar-targets-mean': 0.7586159360352909,\n",
       "   'middle-auc': 0.652702,\n",
       "   'middle-outputs-mean': 0.7072733181067561,\n",
       "   'middle-targets-mean': 0.5,\n",
       "   'permute-auc': 0.6603384999999999,\n",
       "   'permute-outputs-mean': 0.7228447135165861,\n",
       "   'permute-targets-mean': 0.5,\n",
       "   'protherm-outputs-mean': 0.7315263477775036,\n",
       "   'protherm-spearman_corr': -0.021719535832926552,\n",
       "   'protherm-targets-mean': 1.0729990058253231,\n",
       "   'start-auc': 0.643973,\n",
       "   'start-outputs-mean': 0.7163754139248071,\n",
       "   'start-targets-mean': 0.5,\n",
       "   'stop-auc': 0.6309995,\n",
       "   'stop-outputs-mean': 0.727407150736155,\n",
       "   'stop-targets-mean': 0.5},\n",
       "  ('evaluate_validation_dataset', 64): {'edges-auc': 0.981591,\n",
       "   'edges-outputs-mean': 0.4855550451266683,\n",
       "   'edges-targets-mean': 0.5,\n",
       "   'exact-auc': 0.993468,\n",
       "   'exact-outputs-mean': 0.5616812294587562,\n",
       "   'exact-targets-mean': 0.5,\n",
       "   'humsavar-auc': 0.5005309911949798,\n",
       "   'humsavar-outputs-mean': 0.7673011240429986,\n",
       "   'humsavar-targets-mean': 0.7586159360352909,\n",
       "   'middle-auc': 0.988802,\n",
       "   'middle-outputs-mean': 0.4183388922723534,\n",
       "   'middle-targets-mean': 0.5,\n",
       "   'permute-auc': 0.989484,\n",
       "   'permute-outputs-mean': 0.41751613002453813,\n",
       "   'permute-targets-mean': 0.5,\n",
       "   'protherm-outputs-mean': 0.9547958923619132,\n",
       "   'protherm-spearman_corr': -0.0068299321199612175,\n",
       "   'protherm-targets-mean': 1.0729990058253231,\n",
       "   'start-auc': 0.985354,\n",
       "   'start-outputs-mean': 0.43325993758519854,\n",
       "   'start-targets-mean': 0.5,\n",
       "   'stop-auc': 0.962841,\n",
       "   'stop-outputs-mean': 0.5036779342236105,\n",
       "   'stop-targets-mean': 0.5}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JOB_ID = 'job_1'\n",
    "NOTEBOOK_PATH.joinpath(JOB_ID).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script = f\"\"\"\\\n",
    "#!/bin/bash\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --nodes=1\n",
    "# SBATCH --exclusive\n",
    "# SBATCH --mem=0\n",
    "#SBATCH --account=def-pmkim\n",
    "# SBATCH --account=rrg-pmkim\n",
    "#SBATCH --job-name={NOTEBOOK_NAME}\n",
    "#SBATCH --export=ALL\n",
    "#SBATCH --output={NOTEBOOK_PATH.absolute()}/{JOB_ID}/slurm-%A_%a.out\n",
    "#SBATCH --array=0-111\n",
    "set -ev\n",
    "\n",
    "unset XDG_RUNTIME_DIR\n",
    "\n",
    "jupyter nbconvert ./07-{NOTEBOOK_NAME}.ipynb \\\\\n",
    "    --to html \\\\\n",
    "    --execute \\\\\n",
    "    --ExecutePreprocessor.timeout=$((60 * 60 * 24))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with NOTEBOOK_PATH.with_suffix('.sh').open('wt') as fout:\n",
    "    fout.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#SBATCH --time=12:00:00\r\n",
      "#SBATCH --nodes=1\r\n",
      "# SBATCH --exclusive\r\n",
      "# SBATCH --mem=0\r\n",
      "#SBATCH --account=def-pmkim\r\n",
      "# SBATCH --account=rrg-pmkim\r\n",
      "#SBATCH --job-name=validate_trained_network\r\n",
      "#SBATCH --export=ALL\r\n",
      "#SBATCH --output=/gpfs/fs0/scratch/p/pmkim/strokach/datapkg/adjacency-net/notebooks/validate_trained_network/job_1/output.log\r\n",
      "#SBATCH --array=0-111\r\n",
      "set -ev\r\n",
      "\r\n",
      "unset XDG_RUNTIME_DIR\r\n",
      "\r\n",
      "jupyter nbconvert ./07-validate_trained_network.ipynb \\\r\n",
      "    --to html \\\r\n",
      "    --execute \\\r\n",
      "    --ExecutePreprocessor.timeout=$((60 * 60 * 24))\r\n"
     ]
    }
   ],
   "source": [
    "!cat {NOTEBOOK_NAME}.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x {NOTEBOOK_NAME}.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !./{NOTEBOOK_NAME}.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 45974\r\n"
     ]
    }
   ],
   "source": [
    "!sbatch ./{NOTEBOOK_NAME}.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "343px",
    "left": "1654.56px",
    "right": "20px",
    "top": "109.219px",
    "width": "282px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
