{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Submitting jobs\n",
    "\n",
    "**Note:** These jobs must be submitted from the <code>./notebooks</code> folder.\n",
    "\n",
    "**Cedar:**\n",
    "\n",
    "```bash\n",
    "NOTEBOOK_PATH=$(realpath 01_process_pdb_interface.ipynb) sbatch --array=1-300 --time=72:00:00 --nodes=1 --ntasks-per-node=48 --mem=0 --job-name=process-pdb-interface --account=rrg-pmkim --output=/scratch/strokach/tmp/log/run-notebook-cpu-%j-%N.log ../scripts/run_notebook_cpu.sh\n",
    "```\n",
    "\n",
    "### To Do\n",
    "\n",
    "Remove hydrogen atoms on all structures.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import concurrent.futures.process\n",
    "import gzip\n",
    "import importlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import shlex\n",
    "import shutil\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mdtraj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import tenacity\n",
    "import yaml\n",
    "from kmbio import PDB\n",
    "from kmtools import structure_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path(os.getenv(\"CI_JOB_NAME\", \"01_process_pdb_interface\"))\n",
    "\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scinet\" in socket.gethostname():\n",
    "    CPU_COUNT = 40\n",
    "else:\n",
    "    CPU_COUNT = max(1, len(os.sched_getaffinity(0)) // 2)\n",
    "\n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = TASK_ID is None\n",
    "\n",
    "if DEBUG:\n",
    "    TASK_ID = 300\n",
    "    TASK_COUNT = 300\n",
    "else:\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "\n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJACENCY_NET_DATA_PATH = (\n",
    "    Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"adjacency-net-v2\", \"v0.3\").resolve(strict=True)\n",
    ")\n",
    "\n",
    "ADJACENCY_NET_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_DATA_PATH = (\n",
    "    Path(os.environ[\"DATAPKG_OUTPUT_DIR\"]).joinpath(\"pdb-ffindex\", \"2020-01-16\", \"arrow\").resolve(strict=True)\n",
    ")\n",
    "\n",
    "PDB_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PDB_DATA_PATH.joinpath(\"pdb-list.pickle\").open(\"rb\") as fin:\n",
    "    pdb_list = pickle.load(fin)\n",
    "    \n",
    "pdb_data_reader = pa.RecordBatchFileReader(PDB_DATA_PATH.joinpath(\"pdb-mmcif.arrow\"))\n",
    "\n",
    "assert len(pdb_list) == pdb_data_reader.num_record_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = int(np.ceil(len(pdb_list) / TASK_COUNT))\n",
    "task_idx = TASK_ID - 1\n",
    "pdb_chunk = pdb_list[task_idx * chunk_size : (task_idx + 1) * chunk_size]\n",
    "pdb_chunk_idxs = list(range(task_idx * chunk_size, (task_idx + 1) * chunk_size))[:len(pdb_chunk)]\n",
    "assert all(pdb_chunk[i] == pdb_list[j] for i, j in enumerate(pdb_chunk_idxs))\n",
    "\n",
    "chunk_size, task_idx, len(pdb_chunk), pdb_chunk[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    pdb_chunk = pdb_chunk[:10]\n",
    "    pdb_chunk_idxs = pdb_chunk_idxs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_hydrogens(structure_df):\n",
    "    structure_df = structure_df[\n",
    "        ~((structure_df[\"residue_id_0\"].str.strip() == \"\") & structure_df[\"atom_name\"].str.startswith(\"H\"))\n",
    "    ]\n",
    "    return structure_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interacting_chain_pairs(structure_df, distance_cutoff=5, n_residue_pairs_cutoff=5):\n",
    "    # Strip hetatms\n",
    "    structure_df = structure_df[structure_df[\"residue_id_0\"].str.strip() == \"\"]\n",
    "\n",
    "    # Make sure chain ids uniquely identify chains across all models\n",
    "    assert len(structure_df.drop_duplicates([\"model_idx\", \"chain_idx\"])) == structure_df[\"chain_idx\"].nunique()\n",
    "    model_lookup = structure_df.drop_duplicates([\"model_idx\", \"residue_idx\"]).set_index(\"residue_idx\")[\"model_idx\"]\n",
    "\n",
    "    # Make sure residue ids uniquely identify residues across all chains\n",
    "    assert len(structure_df.drop_duplicates([\"chain_idx\", \"residue_idx\"])) == structure_df[\"residue_idx\"].nunique()\n",
    "    chain_lookup = structure_df.drop_duplicates([\"chain_idx\", \"residue_idx\"]).set_index(\"residue_idx\")[\"chain_idx\"]\n",
    "\n",
    "    interactions_df = structure_tools.get_distances(structure_df, max_cutoff=distance_cutoff, groupby=\"residue\")\n",
    "    interactions_df[\"model_idx_1\"] = interactions_df[\"residue_idx_1\"].map(model_lookup)\n",
    "    interactions_df[\"model_idx_2\"] = interactions_df[\"residue_idx_2\"].map(model_lookup)\n",
    "    interactions_df[\"chain_idx_1\"] = interactions_df[\"residue_idx_1\"].map(chain_lookup)\n",
    "    interactions_df[\"chain_idx_2\"] = interactions_df[\"residue_idx_2\"].map(chain_lookup)\n",
    "\n",
    "    chain_pairs = {}\n",
    "    for chain_pair, df in interactions_df.groupby([\"chain_idx_1\", \"chain_idx_2\"], as_index=False):\n",
    "        if chain_pair[0] == chain_pair[1]:\n",
    "            continue\n",
    "        num_interacting_residue_pairs = len(df.drop_duplicates([\"residue_idx_1\", \"residue_idx_2\"]))\n",
    "        if num_interacting_residue_pairs < n_residue_pairs_cutoff:\n",
    "            continue\n",
    "        chain_pairs[chain_pair] = num_interacting_residue_pairs\n",
    "    return chain_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdb_interface(pdb_id, pdb_idx):\n",
    "    # Load data\n",
    "    pdb_data_reader = pa.RecordBatchFileReader(PDB_DATA_PATH.joinpath(\"pdb-mmcif.arrow\"))\n",
    "    pdb_data = pdb_data_reader.get_record_batch(pdb_idx).to_pydict()\n",
    "    assert pdb_data[\"pdb_id\"][0] == pdb_id\n",
    "\n",
    "    # Create structure from data\n",
    "    buf = io.StringIO()\n",
    "    buf.write(gzip.decompress(pdb_data[\"mmcif_data\"][0]).decode())\n",
    "    use_auth_id = False\n",
    "    try:\n",
    "        buf.seek(0)\n",
    "        bioassembly_id = True\n",
    "        structure = PDB.MMCIFParser(use_auth_id=use_auth_id).get_structure(buf, bioassembly_id=bioassembly_id)\n",
    "    except PDB.BioassemblyError as e:\n",
    "        print(f\"Encountered error when parsing pdb {pdb_idx} ('{pdb_id}'): {e!s}.\")\n",
    "        buf.seek(0)\n",
    "        bioassembly_id = False\n",
    "        structure = PDB.MMCIFParser(use_auth_id=use_auth_id).get_structure(buf, bioassembly_id=bioassembly_id)\n",
    "\n",
    "    structure_df = structure.to_dataframe()\n",
    "    structure_df = strip_hydrogens(structure_df)\n",
    "    all_chains = structure_df[\"chain_idx\"].unique().tolist()\n",
    "    all_chain_pairs = [(i, j) for i in all_chains for j in all_chains]\n",
    "    interacting_chain_pairs = get_interacting_chain_pairs(structure_df, distance_cutoff=5, n_residue_pairs_cutoff=5)\n",
    "\n",
    "    results = []\n",
    "    evaluated_sequence_pairs = set()\n",
    "    for chain_idx_1, chain_1 in enumerate(structure.chains):\n",
    "        for chain_idx_2, chain_2 in enumerate(structure.chains):\n",
    "            assert chain_idx_1 in all_chains\n",
    "            assert chain_idx_2 in all_chains\n",
    "\n",
    "            if (chain_idx_1, chain_idx_2) not in interacting_chain_pairs:\n",
    "                continue\n",
    "\n",
    "            aa_sequence_1 = structure_tools.get_chain_sequence(chain_1)\n",
    "            aa_sequence_2 = structure_tools.get_chain_sequence(chain_2)\n",
    "\n",
    "            aa_sequence_pair = (\n",
    "                (aa_sequence_1, aa_sequence_2) if (aa_sequence_1 <= aa_sequence_2) else (aa_sequence_2, aa_sequence_1)\n",
    "            )\n",
    "            if aa_sequence_pair in evaluated_sequence_pairs:\n",
    "                continue\n",
    "            evaluated_sequence_pairs.add(aa_sequence_pair)\n",
    "\n",
    "            if (len(aa_sequence_1.strip()) < 5) or len(aa_sequence_2.strip()) < 5:\n",
    "                continue\n",
    "\n",
    "            # Create a structure with only the two chains of interest\n",
    "            structure_chunk_df = structure_df[\n",
    "                (structure_df[\"chain_idx\"] == chain_idx_1) | (structure_df[\"chain_idx\"] == chain_idx_2)\n",
    "            ].copy()\n",
    "            chain_idx_array = (\n",
    "                structure_chunk_df.drop_duplicates(\"residue_idx\")[\"chain_idx\"]\n",
    "                .map({c: i for i, c in enumerate(structure_chunk_df[\"chain_idx\"].unique())})\n",
    "                .values\n",
    "            )\n",
    "            structure_chunk_df[\"model_idx\"] = 0\n",
    "            structure_chunk_df[\"model_id\"] = 0\n",
    "            structure_chunk_df[\"chain_idx\"] = 0\n",
    "            structure_chunk_df[\"chain_id\"] = \"A\"\n",
    "            structure_chunk_df[\"residue_id_1\"] = (\n",
    "                structure_chunk_df[\"residue_idx\"]\n",
    "                .map({r: i for i, r in enumerate(structure_chunk_df[\"residue_idx\"].unique())})\n",
    "                .values\n",
    "            )\n",
    "\n",
    "            # Convert to mdtraj trajectory\n",
    "            schain = PDB.Structure.from_dataframe(structure_chunk_df)\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdb\") as pdb_file:\n",
    "                PDB.save(schain, pdb_file.name)\n",
    "                traj = mdtraj.load(pdb_file.name)\n",
    "            assert (aa_sequence_1 + aa_sequence_2) == traj.top.to_fasta()[0]\n",
    "\n",
    "            residue_df = helper.construct_residue_df(traj)\n",
    "            assert len(residue_df) == len(chain_idx_array)\n",
    "            residue_df[\"chain_idx\"] = chain_idx_array\n",
    "            helper.validate_residue_df(residue_df)\n",
    "\n",
    "            residue_pairs_df = helper.construct_residue_pairs_df(traj)\n",
    "            helper.validate_residue_pairs_df(residue_pairs_df)\n",
    "\n",
    "            result = {\n",
    "                \"pdb_id\": [pdb_id],\n",
    "                \"pdb_idx\": [pdb_idx],\n",
    "                \"use_auth_id\": [use_auth_id],\n",
    "                \"bioassembly_id\": [bioassembly_id],\n",
    "                \"model_id_1\": [chain_1.parent.id],\n",
    "                \"chain_idx_1\": [chain_idx_1],\n",
    "                \"chain_id_1\": [chain_1.id],\n",
    "                \"chain_idx_2\": [chain_idx_2],\n",
    "                \"chain_id_2\": [chain_2.id],\n",
    "                \"num_interacting_residue_pairs\": [interacting_chain_pairs[(chain_idx_1, chain_idx_2)]],\n",
    "                \"aa_sequence_1\": [aa_sequence_1],\n",
    "                \"aa_sequence_2\": [aa_sequence_2],\n",
    "                **helper.residue_df_to_row(residue_df),\n",
    "                **helper.residue_pairs_df_to_row(residue_pairs_df),\n",
    "            }\n",
    "            result = helper.downcast_and_compress(result)\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(pdb_id, pdb_idx):\n",
    "    try:\n",
    "        results = process_pdb_interface(pdb_id, pdb_idx)\n",
    "        return results, []\n",
    "    except Exception as error:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback_string = \"\\n\".join(traceback.format_exception(exc_type, exc_value, exc_traceback))\n",
    "        failure = {\n",
    "            \"pdb_id\": [pdb_id],\n",
    "            \"pdb_idx\": [pdb_idx],\n",
    "            \"error_type\": [str(type(error))],\n",
    "            \"error_message\": [str(error)],\n",
    "            \"error_traceback\": [traceback_string],\n",
    "        }\n",
    "        return [], [failure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = process_pdb_interface(\"1orf\", 17384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with concurrent.futures.ProcessPoolExecutor() as pool:\n",
    "#     futures = pool.map(worker, pdb_chunk[:10], pdb_chunk_idxs[:10])\n",
    "#     for result in tqdm(futures, total=100):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = ADJACENCY_NET_DATA_PATH.joinpath(\"pdb-interface\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir.joinpath(f\"pdb-interface-{TASK_ID}-{TASK_COUNT}.arrow\")\n",
    "\n",
    "output_dir_failed = output_dir.joinpath(\"failed\")\n",
    "output_dir_failed.mkdir(exist_ok=True)\n",
    "output_file_failed = output_dir_failed.joinpath(f\"pdb-interface-{TASK_ID}-{TASK_COUNT}-failed.arrow\")\n",
    "\n",
    "output_file, output_file_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"kmtools.structure_tools.fixes\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = None\n",
    "writer_failed = None\n",
    "num_pdbs_processed = 0\n",
    "while num_pdbs_processed < len(pdb_chunk):\n",
    "    try:\n",
    "        with concurrent.futures.ProcessPoolExecutor(CPU_COUNT) as pool:\n",
    "            futures = pool.map(worker, pdb_chunk[num_pdbs_processed:], pdb_chunk_idxs[num_pdbs_processed:], chunksize=1)\n",
    "            for (results, results_failed) in tqdm(futures, total=len(pdb_chunk) - num_pdbs_processed):\n",
    "                num_pdbs_processed += 1\n",
    "                for result in results:\n",
    "                    if writer is None:\n",
    "                        batch = pa.RecordBatch.from_arrays(list(result.values()), list(result.keys()))\n",
    "                        writer = pa.RecordBatchFileWriter(output_file, batch.schema)\n",
    "                    batch = pa.RecordBatch.from_arrays(list(result.values()), list(result.keys()))\n",
    "                    writer.write_batch(batch)\n",
    "                for result_failed in results_failed:\n",
    "                    if writer_failed is None:\n",
    "                        batch = pa.RecordBatch.from_arrays(list(result_failed.values()), list(result_failed.keys()))\n",
    "                        writer_failed = pa.RecordBatchFileWriter(output_file_failed, batch.schema)\n",
    "                    batch = pa.RecordBatch.from_arrays(list(result_failed.values()), list(result_failed.keys()))\n",
    "                    writer_failed.write_batch(batch)\n",
    "    except concurrent.futures.BrokenExecutor as e:\n",
    "        print(f\"ProcessPoolExecutor crashed with an error ('{type(e)!s}'): '{e!s}'.\")\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "if writer_failed is not None:\n",
    "    writer_failed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_file.is_file():\n",
    "    reader = pa.RecordBatchFileReader(output_file)\n",
    "    print(f\"Number of successful chains: {reader.num_record_batches}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_file_failed.is_file():\n",
    "    reader_failed = pa.RecordBatchFileReader(output_file_failed)\n",
    "    print(f\"Number of failed PDBs: {reader_failed.num_record_batches}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a `*.SUCCESS` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with output_file.with_suffix(\".SUCCESS\").open(\"wt\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482px",
    "left": "34.9826px",
    "top": "126.979px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
