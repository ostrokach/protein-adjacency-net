{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Create validation dataset measuring decoy discrimination accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import importlib\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from boltons import strutils\n",
    "\n",
    "import kmbio.PDB\n",
    "from kmtools import structure_tools\n",
    "from tkpod.plugins.modeller import Modeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path('decoy_discrimination_dataset')\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    TASK_ID = 1\n",
    "    TASK_COUNT = 200\n",
    "else:\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "    \n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (\n",
    "    NOTEBOOK_PATH.name + \n",
    "    (\"\" if TASK_ID is None else f\"-{TASK_ID:03}\") +\n",
    "    \".parquet\"\n",
    ")\n",
    "OUTPUT_FILE = OUTPUT_PATH.joinpath(filename)\n",
    "OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DATA['3drobot'] = (\n",
    "    Path(os.environ[\"DATAPKG_INPUT_DIR\"])\n",
    "    .joinpath(\"3drobot\", \"2018-11-16\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(INPUT_DATA['3drobot'].glob(\"*.tar.bz2\"))[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_one(it):\n",
    "    vals = list(it)\n",
    "    assert len(vals) == 1\n",
    "    return vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_one(it):\n",
    "    items = list(it)\n",
    "    assert len(items) == 1, items\n",
    "    item = items[0]\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_fh(text):\n",
    "    fh = io.StringIO()\n",
    "    fh.write(text)\n",
    "    fh.seek(0)\n",
    "    return fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fh_to_structure(fh):\n",
    "    parser = kmbio.PDB.PDBParser()\n",
    "    structure = parser.get_structure(fh, bioassembly_id=False)\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_modeller_scores(structure, chain_id, sequence):\n",
    "    target = structure_tools.DomainTarget(0, chain_id, sequence, 1, len(sequence), sequence)\n",
    "    modeller_data = Modeller.build(structure, bioassembly_id=False, use_strict_alignment=True)\n",
    "    structure_bm, modeller_results = Modeller.create_model([target], modeller_data)\n",
    "    modeller_results = {k.replace(\" \", \"_\").lower(): v for k, v in modeller_results.items()}\n",
    "    # Format GA341 score\n",
    "    for i in range(len(modeller_results['ga341_score'])):\n",
    "        modeller_results[f'ga341_score_{i}'] = modeller_results['ga341_score'][i]\n",
    "    modeller_results['ga341_score'] = modeller_results['ga341_score_0']\n",
    "    # Format pdfterms\n",
    "    modeller_results.update({\n",
    "        \"modeller_\" + strutils.slugify(repr(k)): v\n",
    "        for k, v\n",
    "        in dict(modeller_results['pdfterms']).items()\n",
    "    })\n",
    "    del modeller_results['pdfterms']\n",
    "    return modeller_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_structure_info(row):\n",
    "    results = {}\n",
    "    \n",
    "    fh = text_to_fh(row.structure_text)\n",
    "    structure = fh_to_structure(fh)\n",
    "    structure.id = row.unique_id\n",
    "    sequence = structure_tools.get_chain_sequence(structure[0][row.chain_id])\n",
    "\n",
    "    # Modeller\n",
    "    modeller_scores = get_modeller_scores(structure, row.chain_id, sequence)\n",
    "    results.update(modeller_scores)\n",
    "\n",
    "    # Adj. mat\n",
    "    ic, ica = helper.get_homology_model_interactions(row)\n",
    "    residue_pairs = ica.at[0, \"residue_pair\"]\n",
    "    residue_idx_1, residue_idx_2 = list(zip(*[t for t in residue_pairs if t[0] != t[1]]))\n",
    "    results.update({\n",
    "        \"sequence\": sequence,\n",
    "        \"residue_idx_1\": list(residue_idx_1),\n",
    "        \"residue_idx_2\": list(residue_idx_2),\n",
    "    })\n",
    "\n",
    "    # Rosetta (most time-consuming, so do last)\n",
    "    rosetta_scores = helper.get_rosetta_scores(row)\n",
    "    results.update(rosetta_scores)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def worker(row_dict):\n",
    "    row = helper.to_namedtuple(row_dict)\n",
    "    try:\n",
    "        results = {\n",
    "            \"index\": row.Index,\n",
    "            \"unique_id\": row.unique_id,\n",
    "            \"error\": None,\n",
    "            **get_structure_info(row),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results = {\n",
    "            \"index\": row.Index,\n",
    "            \"unique_id\": row.unique_id,\n",
    "            \"error\": f\"{type(e)}: {e}\",\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of decoy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = sorted(INPUT_DATA['3drobot'].glob(\"*.tar.bz2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_ID and TASK_COUNT:\n",
    "    assert len(files) == TASK_COUNT\n",
    "    files = files[TASK_ID - 1 : TASK_ID]\n",
    "    \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs =[]\n",
    "for file in files:\n",
    "    filename = file.name.split('.')[0]\n",
    "    assert len(filename) == 5\n",
    "    pdb_id, chain_id = filename[:4], filename[4]\n",
    "\n",
    "    tempdir = tempfile.TemporaryDirectory()\n",
    "    tempdir_name = tempdir.name\n",
    "    tempdir_name = tempfile.mkdtemp()\n",
    "    with tarfile.open(file, \"r:bz2\") as tar:\n",
    "        tar.extractall(tempdir_name)\n",
    "    \n",
    "    df = pd.read_csv(Path(tempdir_name).joinpath(filename, \"list.txt\"), sep=' +', engine=\"python\")\n",
    "    df.rename(columns={\"NAME\": \"decoy_name\", \"RMSD\": \"rmsd\"}, inplace=True)\n",
    "    df.set_index(\"decoy_name\", inplace=True)\n",
    "    df[\"structure_text\"] = None\n",
    "    assert df['structure_text'].isnull().all()\n",
    "    for pdb_file in Path(tempdir_name).joinpath(filename).glob(\"*.pdb\"):\n",
    "        with pdb_file.open(\"rt\") as fin:\n",
    "            df.loc[pdb_file.name, \"structure_text\"] = fin.read()\n",
    "    assert df['structure_text'].notnull().all()\n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"structure_id\"] = filename\n",
    "    df[\"pdb_id\"] = pdb_id\n",
    "    df[\"chain_id\"] = [(chain_id if n == \"native.pdb\" else \" \") for n in df[\"decoy_name\"]]\n",
    "    df[\"unique_id\"] = filename + \"-\" + df[\"decoy_name\"].str.split('.').str[0]\n",
    "    dfs.append(df)\n",
    "    tempdir.cleanup()\n",
    "    \n",
    "dataset = pd.concat(dfs, ignore_index=True)\n",
    "assert len(dataset[\"unique_id\"]) == len(dataset[\"unique_id\"].drop_duplicates())\n",
    "assert len(dataset.index) == len(set(dataset.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = list(dataset.itertuples())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_structure_info(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    dataset = dataset.iloc[:psutil.cpu_count(logical=False)]\n",
    "    \n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with concurrent.futures.ProcessPoolExecutor(psutil.cpu_count(logical=False)) as pool:\n",
    "    futures = pool.map(worker, (t._asdict() for t in dataset.itertuples()))\n",
    "    results = list(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).set_index(\"index\")\n",
    "results_df.rename(columns=lambda s: s.replace(\" \", \"_\").lower(), inplace=True)\n",
    "display(results_df.head(4))\n",
    "print(\"Number of rows: \", results_df.shape[0])\n",
    "print(\"Number of errors: \", sum(results_df['error'].notnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_wresults = (\n",
    "    dataset\n",
    "    .merge(results_df, left_index=True, right_index=True, validate=\"1:1\", copy=False, suffixes=(\"\", \"_copy\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in dataset_wresults.columns:\n",
    "    if col.endswith(\"_copy\"):\n",
    "        col_ref = col[:-5]\n",
    "        assert (dataset_wresults[col] == dataset_wresults[col_ref]).all()\n",
    "        del dataset_wresults[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse failed subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wresults_failed = dataset_wresults[dataset_wresults['error'].notnull()]\n",
    "\n",
    "display(dataset_wresults_failed.head(2))\n",
    "print(dataset_wresults_failed.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(dataset_wresults_failed, preserve_index=True)\n",
    "pq.write_table(table, OUTPUT_FILE.with_suffix(\".failed\"), version=\"2.0\", flavor=\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse successful subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wresults_succeeded = dataset_wresults[dataset_wresults['error'].isnull()]\n",
    "\n",
    "display(dataset_wresults_succeeded.head(2))\n",
    "print(dataset_wresults_succeeded.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(dataset_wresults_succeeded, preserve_index=True)\n",
    "pq.write_table(table, OUTPUT_FILE, version=\"2.0\", flavor=\"spark\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
