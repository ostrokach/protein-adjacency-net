{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shelve\n",
    "import socket\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path(os.getenv(\"CI_JOB_NAME\", \"04_add_adjacency_distances_validation\"))\n",
    "\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scinet\" in socket.gethostname():\n",
    "    CPU_COUNT = 40\n",
    "else:\n",
    "    CPU_COUNT = max(1, len(os.sched_getaffinity(0)) // 2)\n",
    "\n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = TASK_ID is None\n",
    "\n",
    "if DEBUG:\n",
    "    TASK_ID = 216\n",
    "    TASK_COUNT = 1027\n",
    "else:\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "\n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJACENCY_MATRIX_PARQUET_PATH = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\n",
    "    \"adjacency-net-v2\", \"v0.3\", \"training_dataset\", \"adjacency_matrix.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted([d for d in ADJACENCY_MATRIX_PARQUET_PATH.glob(\"database_id=*\") if d.is_dir()])\n",
    "folders[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders.index(Path(\"/scratch/strokach/datapkg_output_dir/adjacency-net-v2/v0.3/training_dataset/adjacency_matrix.parquet/database_id=G3DSA%3A1.20.120.420\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(folders[TASK_ID - 1].glob(\"*.parquet\"))\n",
    "\n",
    "print(files[:2])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pq.ParquetFile(files[0])\n",
    "    .read_row_group(0, columns=[\"__index_level_0__\"], use_pandas_metadata=False)\n",
    "    .to_pandas(integer_object_nulls=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find successful jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file(file, failed=False):\n",
    "    file_parts = list(file.parts)\n",
    "    file_parts[-4] = file_parts[-4] + \"_wdistances\"\n",
    "    file_parts[-1] = file_parts[-1].split(\".\")[0] + \".arrow\"\n",
    "    if failed:\n",
    "        file_parts.insert(-3, \"failed\")\n",
    "    new_file = Path(*file_parts)\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succeeded = {}\n",
    "\n",
    "for task_id in tqdm(range(1, TASK_COUNT + 1)):\n",
    "    task_idx = task_id - 1\n",
    "    files = sorted(folders[task_idx].glob(\"*.parquet\"))\n",
    "\n",
    "    succeeded[task_id] = True\n",
    "    for file in files:\n",
    "        new_file = get_new_file(file)\n",
    "        if not new_file.parent.joinpath(\"._SUCCESS\").is_file():\n",
    "            succeeded[task_id] = False\n",
    "            break\n",
    "        if not new_file.is_file():\n",
    "            succeeded[task_id] = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for v in succeeded.values() if v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for v in succeeded.values() if not v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \",\".join([str(k) for k, v in status.items() if v == \"failure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze successful jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_cache_file = OUTPUT_PATH.joinpath(\"stats.cache\")\n",
    "\n",
    "stats_all = {}\n",
    "with shelve.open(stats_cache_file.as_posix()) as stats_cache:\n",
    "    for key in stats_cache:\n",
    "        stats_all[int(key)] = stats_cache[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id in tqdm(range(1, TASK_COUNT + 1)):\n",
    "    if task_id in stats_all:\n",
    "        continue\n",
    "\n",
    "    stats = {\n",
    "        \"succeeded\": succeeded[task_id],\n",
    "        \"succeeded_indices\": set(),\n",
    "        \"failed_indices\": set(),\n",
    "        \"missing_indices\": set(),\n",
    "    }\n",
    "\n",
    "    if stats[\"succeeded\"]:\n",
    "        files = sorted(folders[task_id - 1].glob(\"*.parquet\"))\n",
    "        try:\n",
    "            for file in files:\n",
    "                all_indices = set(\n",
    "                    pq.read_table(file, columns=[\"__index_level_0__\"], use_pandas_metadata=False)\n",
    "                    .to_pandas(integer_object_nulls=True)[\"__index_level_0__\"]\n",
    "                    .values.tolist()\n",
    "                )\n",
    "\n",
    "                new_file = get_new_file(file)\n",
    "                if new_file.is_file():\n",
    "                    reader = pa.RecordBatchFileReader(new_file)\n",
    "                    for record_batch_idx in tqdm(range(reader.num_record_batches), leave=False):\n",
    "                        batch = reader.get_record_batch(record_batch_idx)\n",
    "                        index = batch.column(22)[0]\n",
    "                        stats[\"succeeded_indices\"].add(index.as_py())\n",
    "                assert not stats[\"succeeded_indices\"] - all_indices\n",
    "\n",
    "                new_file = get_new_file(file, failed=True)\n",
    "                if new_file.is_file():\n",
    "                    reader = pa.RecordBatchFileReader(new_file)\n",
    "                    for record_batch_idx in tqdm(range(reader.num_record_batches), leave=False):\n",
    "                        batch = reader.get_record_batch(record_batch_idx)\n",
    "                        index = batch.to_pydict()[\"Index\"][0]\n",
    "                        stats[\"failed_indices\"].add(index)\n",
    "                assert not stats[\"failed_indices\"] - all_indices\n",
    "\n",
    "                assert not stats[\"succeeded_indices\"] & stats[\"failed_indices\"]\n",
    "\n",
    "                stats[\"missing_indices\"] = all_indices - stats[\"succeeded_indices\"] - stats[\"failed_indices\"]\n",
    "        except pa.ArrowInvalid:\n",
    "            stats[\"succeeded\"] = False\n",
    "\n",
    "    stats_all[task_id] = stats\n",
    "    with shelve.open(stats_cache_file.as_posix()) as stats_cache:\n",
    "        stats_cache[str(task_id)] = stats_all[task_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
