{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import concurrent.futures.process\n",
    "import importlib\n",
    "import os\n",
    "import shlex\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from kmbio import PDB\n",
    "from kmtools import structure_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path(os.getenv(\"CI_JOB_NAME\", \"03_add_adjacency_distances\"))\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd().expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "ADJACENCY_MATRIX_PARQUET_PATH = os.getenv(\"ADJACENCY_MATRIX_PARQUET_PATH\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "ADJACENCY_MATRIX_PARQUET_PATH = (\n",
    "    Path(ADJACENCY_MATRIX_PARQUET_PATH).expanduser() if ADJACENCY_MATRIX_PARQUET_PATH is not None else None\n",
    ")\n",
    "\n",
    "TASK_ID, TASK_COUNT, ADJACENCY_MATRIX_PARQUET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "\n",
    "if DEBUG:\n",
    "    TASK_ID = 78\n",
    "    TASK_COUNT = 1029\n",
    "    ADJACENCY_MATRIX_PARQUET_PATH = (\n",
    "        Path(os.getenv(\"DATAPKG_OUTPUT_DIR\"))\n",
    "        .joinpath(\"adjacency-net-v2\", \"v0.3\", \"training_dataset\", \"adjacency_matrix.parquet\")\n",
    "    )\n",
    "else:\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "    assert ADJACENCY_MATRIX_PARQUET_PATH is not None\n",
    "\n",
    "assert ADJACENCY_MATRIX_PARQUET_PATH.is_dir()\n",
    "\n",
    "TASK_ID, TASK_COUNT, ADJACENCY_MATRIX_PARQUET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG[\"pdb-ffindex\"] = {\n",
    "    \"pdb_mmcif_ffindex\": (\n",
    "        Path(os.environ[\"DATAPKG_OUTPUT_DIR\"]).joinpath(\"pdb-ffindex\", \"2018-09-06\", \"pdb-mmcif\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([f for f in ADJACENCY_MATRIX_PARQUET_PATH.glob(\"**/*.parquet\") if f.is_file()])\n",
    "\n",
    "print(files[:2])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{f.parent.parent for f in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = int(np.ceil(len(files) / TASK_COUNT))\n",
    "if len(files) > chunk_size:\n",
    "    files = files[(TASK_ID - 1) * chunk_size:TASK_ID * chunk_size]\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pq.ParquetFile(files[0])\n",
    "    .read_row_group(0, use_pandas_metadata=True)\n",
    "    .to_pandas(integer_object_nulls=True)\n",
    "    .set_index(\"__index_level_0__\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(islice(df.itertuples(), 3))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRUCTURE_URL_PREFIX = f\"ff://{DATAPKG['pdb-ffindex']['pdb_mmcif_ffindex']}?\"\n",
    "STRUCTURE_URL_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = helper.get_adjacency_with_distances_and_orientations(\n",
    "    row, max_cutoff=12, min_cutoff=None, structure_url_prefix=STRUCTURE_URL_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = results[\"distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots()\n",
    "ax.hist(ar.to_pylist(), range=(0, 12), bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.RecordBatch.from_arrays(list(results.values()), list(results.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test as part of a multiprocessing worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(data):\n",
    "    row = helper.to_namedtuple(data)\n",
    "\n",
    "    results = None\n",
    "    failures = None\n",
    "\n",
    "    try:\n",
    "        results = helper.get_adjacency_with_distances_and_orientations(\n",
    "            row, max_cutoff=12, min_cutoff=None, structure_url_prefix=STRUCTURE_URL_PREFIX\n",
    "        )\n",
    "    except Exception as e:\n",
    "        failures = {\"error\": pa.array([f\"{type(e)}: {e}\"])}\n",
    "\n",
    "    for column in [\n",
    "        \"Index\",\n",
    "        \"uniparc_id\",\n",
    "        \"sequence\",\n",
    "        \"database\",\n",
    "        \"interpro_name\",\n",
    "        \"interpro_id\",\n",
    "        \"domain_start\",\n",
    "        \"domain_end\",\n",
    "        \"domain_length\",\n",
    "        \"structure_id\",\n",
    "        \"model_id\",\n",
    "        \"chain_id\",\n",
    "        \"pc_identity\",\n",
    "        \"alignment_length\",\n",
    "        \"mismatches\",\n",
    "        \"gap_opens\",\n",
    "        \"q_start\",\n",
    "        \"q_end\",\n",
    "        \"s_start\",\n",
    "        \"s_end\",\n",
    "        \"evalue_log10\",\n",
    "        \"bitscore\",\n",
    "        \"qseq\",\n",
    "        \"sseq\",\n",
    "    ]:\n",
    "        if results is not None:\n",
    "            results[column] = pa.array([data[column]])\n",
    "        if failures is not None:\n",
    "            failures[column] = pa.array([data[column]])\n",
    "\n",
    "    for column in [\"a2b\", \"b2a\", \"residue_id_1\", \"residue_id_2\", \"residue_aa_1\", \"residue_aa_2\"]:\n",
    "        if data[column].dtype in (int, float):\n",
    "            values = [(int(i) if pd.notnull(i) else None) for i in data[column]]\n",
    "        else:\n",
    "            values = data[column].tolist()\n",
    "        if results is not None:\n",
    "            results[column] = pa.array([values])\n",
    "        if failures is not None:\n",
    "            failures[column] = pa.array([values])\n",
    "\n",
    "    return results, failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker(row._asdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file(file, failed=False):\n",
    "    file_parts = list(file.parts)\n",
    "    file_parts[-4] = file_parts[-4] + \"_wdistances\" + (\"_failed\" if failed else \"\")\n",
    "    new_file = Path(*file_parts)\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_processed = 0\n",
    "for file in tqdm(files):\n",
    "    ds = pq.ParquetFile(file)\n",
    "\n",
    "    new_file = get_new_file(file)\n",
    "    new_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    writer = None\n",
    "\n",
    "    new_file_failed = get_new_file(file, failed=True)\n",
    "    new_file_failed.parent.mkdir(parents=True, exist_ok=True)\n",
    "    writer_failed = None\n",
    "\n",
    "    for row_group in tqdm(range(ds.num_row_groups), leave=False):\n",
    "        df = (\n",
    "            ds.read_row_group(row_group, use_pandas_metadata=True)\n",
    "            .to_pandas(integer_object_nulls=True)\n",
    "            .set_index(\"__index_level_0__\")\n",
    "        )\n",
    "        try:\n",
    "            with concurrent.futures.ProcessPoolExecutor(psutil.cpu_count(logical=False)) as pool:\n",
    "                futures = pool.map(worker, (t._asdict() for t in df.itertuples()), chunksize=1)\n",
    "                results, failures = list(zip(*list(tqdm(futures, leave=False, total=len(df)))))\n",
    "\n",
    "            num_failures = sum(1 for r in results if r is None)\n",
    "            results = [r for r in results if r is not None]\n",
    "            failures = [f for f in failures if f is not None]\n",
    "            assert len(failures) == num_failures\n",
    "            if num_failures:\n",
    "                print(f\"Encountered {num_failures} errors when parsing file '{file}'.\")\n",
    "            n_rows_processed += len(df)\n",
    "        except concurrent.futures.process.BrokenProcessPool as e:\n",
    "            warnings.warn(\n",
    "                f\"ProcessPool crashed while processing row_group '{row_group}' in file '{file}'.\"\n",
    "                f\"The error is '{type(e)}': {e}.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        if results:\n",
    "            if writer is None:\n",
    "                result = results[0]\n",
    "                batch = pa.RecordBatch.from_arrays(list(result.values()), list(result.keys()))\n",
    "                writer = pa.RecordBatchFileWriter(new_file, batch.schema)\n",
    "            for result in results:\n",
    "                batch = pa.RecordBatch.from_arrays(list(result.values()), list(result.keys()))\n",
    "                writer.write_batch(batch)\n",
    "\n",
    "        if failures:\n",
    "            if writer_failed is None:\n",
    "                failure = failures[0]\n",
    "                batch = pa.RecordBatch.from_arrays(list(failure.values()), list(failure.keys()))\n",
    "                writer_failed = pa.RecordBatchFileWriter(new_file_failed, batch.schema)\n",
    "        for failure in failures:\n",
    "            batch = pa.RecordBatch.from_arrays(list(failure.values()), list(failure.keys()))\n",
    "            writer_failed.write_batch(batch)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    if writer_failed is not None:\n",
    "        writer_failed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that everything went ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reader = pa.RecordBatchFileReader(new_file)\n",
    "except pa.ArrowIOError:\n",
    "    num_successful_batches = 0\n",
    "else:\n",
    "    num_successful_batches = reader.num_record_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reader_failed = pa.RecordBatchFileReader(new_file_failed)\n",
    "except pa.ArrowIOError:\n",
    "    num_failed_batches = 0\n",
    "else:\n",
    "    num_failed_batches = reader_failed.num_record_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (num_successful_batches + num_failed_batches) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482px",
    "left": "34.9826px",
    "top": "126.979px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
