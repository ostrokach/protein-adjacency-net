{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting the PACKAGE_NAME environment variable.\n",
      "Setting the PACKAGE_VERSION environment variable.\n",
      "Setting the DOCS_SECRET_KEY environment variable.\n",
      "Setting the PYTHON_VERSION environment variable.\n",
      "Setting the SPARK_MASTER environment variable.\n",
      "Setting the SPARK_ARGS environment variable.\n",
      "Setting the DB_TYPE environment variable.\n",
      "Setting the DB_PORT environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-25 17:44:58.774187\n"
     ]
    }
   ],
   "source": [
    "%run _imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kimlab2/database_data/databin/uniparc_domain/0.1/adjacency_matrix.parquet\n"
     ]
    }
   ],
   "source": [
    "%run _settings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.6.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.6.210:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>adjacency-net</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcc037d0b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run _spark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = 'threshold_by_pc_identity'\n",
    "NOTEBOOK_PATH = Path(NOTEBOOK_NAME).absolute()\n",
    "\n",
    "NOTEBOOK_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene3D domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(f'generate_datasets/gene3d_domains.pickle', 'rb') as fin:\n",
    "    GENE3D_DOMAINS = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / validation domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(f'generate_datasets/training_domains.pickle', 'rb') as fin:\n",
    "    TRAINING_DOMAINS = pickle.load(fin)\n",
    "    \n",
    "with open(f'generate_datasets/validation_domains.pickle', 'rb') as fin:\n",
    "    VALIDATION_DOMAINS = pickle.load(fin)\n",
    "    \n",
    "with open(f'generate_datasets/test_domains.pickle', 'rb') as fin:\n",
    "    TEST_DOMAINS = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / validation parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(f'generate_datasets/training_parquet_files.pickle', 'rb') as fin:\n",
    "    TRAINING_PARQUET_FILES = pickle.load(fin)\n",
    "    \n",
    "with open(f'generate_datasets/validation_parquet_files.pickle', 'rb') as fin:\n",
    "    VALIDATION_PARQUET_FILES = pickle.load(fin)\n",
    "    \n",
    "with open(f'generate_datasets/test_parquet_files.pickle', 'rb') as fin:\n",
    "    TEST_PARQUET_FILES = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL where strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DOMAINS_STRING = \"('{}')\".format(\"', '\".join(urllib.parse.unquote(d)[12:] for d in TRAINING_DOMAINS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_DOMAINS_STRING = \"('{}')\".format(\"', '\".join(urllib.parse.unquote(d)[12:] for d in VALIDATION_DOMAINS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_DOMAINS_STRING = \"('{}')\".format(\"', '\".join(urllib.parse.unquote(d)[12:] for d in TEST_DOMAINS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('G3DSA:2.40.128.20', 'G3DSA:3.50.40.10', 'G3DSA:2.60.40.830', 'G3DSA:2.20.50.20', 'G3DSA:2.40.50.240', 'G3DSA:1.10.569.10', 'G3DSA:2.40.40.10', 'G3DSA:4.10.1240.10', 'G3DSA:4.10.1080.10', 'G3DSA:3.90.1170.20', 'G3DSA:3.30.70.80', 'G3DSA:4.10.20.10', 'G3DSA:1.20.1520.10', 'G3DSA:3.30.1120.40', 'G3DSA:3.90.1650.10', 'G3DSA:1.10.250.10', 'G3DSA:1.10.1390.10', 'G3DSA:3.40.1550.10', 'G3DSA:2.60.490.10', 'G3DSA:1.10.530.40', 'G3DSA:3.30.910.10', 'G3DSA:1.20.1480.10', 'G3DSA:3.30.1390.20', 'G3DSA:1.10.150.170', 'G3DSA:1.10.238.80', 'G3DSA:3.90.1360.10', 'G3DSA:1.20.90.10', 'G3DSA:3.30.920.20', 'G3DS\n"
     ]
    }
   ],
   "source": [
    "print(TRAINING_DOMAINS_STRING[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['database_id=G3DSA%3A1.10.10.400',\n",
       " 'database_id=G3DSA%3A2.40.128.120',\n",
       " 'database_id=G3DSA%3A4.10.800.10',\n",
       " 'database_id=G3DSA%3A1.20.10.10',\n",
       " 'database_id=G3DSA%3A3.40.50.1260',\n",
       " 'database_id=G3DSA%3A2.60.40.1200',\n",
       " 'database_id=G3DSA%3A1.20.5.1010',\n",
       " 'database_id=G3DSA%3A1.20.900.10',\n",
       " 'database_id=G3DSA%3A1.20.5.210',\n",
       " 'database_id=G3DSA%3A1.20.5.260']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(ADJACENCY_MATRIX_PATH)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_strings = {\n",
    "#     'training': TRAINING_DOMAINS_STRING,\n",
    "#     'validation': VALIDATION_DOMAINS_STRING,\n",
    "#     'test': TEST_DOMAINS_STRING,\n",
    "# }\n",
    "\n",
    "# seen = {\n",
    "# }\n",
    "\n",
    "# for subset in ['training', 'validation', 'test']:\n",
    "#     for cutoff in [0, 40, 60, 80]:\n",
    "#         print(subset, cutoff, flush=True)\n",
    "#         if (subset, cutoff) in seen:\n",
    "#             print(\"skipping...\")\n",
    "#             continue\n",
    "#         domain_string = domain_strings[subset]\n",
    "#         output_path = NOTEBOOK_PATH.joinpath(f'adjacency_matrix_{subset}_gt{cutoff}.parquet').absolute()\n",
    "#         output_path.mkdir(parents=True, exist_ok=True)\n",
    "#         query = spark.sql(f\"\"\"\\\n",
    "#             SELECT *\n",
    "#             FROM parquet.`{ADJACENCY_MATRIX_PATH}`\n",
    "#             WHERE database_id in {domain_string}\n",
    "#             AND pc_identity > {cutoff}\n",
    "#         \"\"\")\n",
    "#         query.write.parquet(\n",
    "#             output_path.as_posix(),\n",
    "#             mode='overwrite',\n",
    "#             partitionBy='database_id',\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0\n",
      "training 40\n",
      "training 60\n",
      "training 80\n",
      "validation 0\n",
      "validation 40\n",
      "validation 60\n",
      "validation 80\n",
      "test 0\n",
      "test 40\n",
      "test 60\n",
      "test 80\n"
     ]
    }
   ],
   "source": [
    "domain_strings = {\n",
    "    'training': TRAINING_DOMAINS_STRING,\n",
    "    'validation': VALIDATION_DOMAINS_STRING,\n",
    "    'test': TEST_DOMAINS_STRING,\n",
    "}\n",
    "\n",
    "seen = {\n",
    "    ('training', 0)\n",
    "}\n",
    "\n",
    "for subset in ['training', 'validation', 'test']:\n",
    "    for cutoff in [0, 40, 60, 80]:\n",
    "        print(subset, cutoff, flush=True)\n",
    "        if (subset, cutoff) in seen:\n",
    "            print(\"skipping...\")\n",
    "            continue\n",
    "        domain_string = domain_strings[subset]\n",
    "        output_path = NOTEBOOK_PATH.joinpath(f'adjacency_matrix_{subset}_gt{cutoff}.parquet').absolute()\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        count1 = spark.sql(f\"\"\"\\\n",
    "            SELECT COUNT(*)\n",
    "            FROM parquet.`{ADJACENCY_MATRIX_PATH}`\n",
    "            WHERE database_id in {domain_string}\n",
    "            AND pc_identity > {cutoff}\n",
    "        \"\"\").take(1)\n",
    "        count2 = spark.sql(f\"\"\"\\\n",
    "            SELECT COUNT(*)\n",
    "            FROM parquet.`{output_path}`\n",
    "        \"\"\").take(1)\n",
    "        assert count1[0]['count(1)'] == count2[0]['count(1)']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "404px",
    "left": "1660.26px",
    "right": "20px",
    "top": "106.354px",
    "width": "387px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
