{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import itertools\n",
    "import importlib\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from kmtools import py_tools, sequence_tools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numba import njit, prange\n",
    "from scipy import stats\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import pagnn.models.dcn\n",
    "from pagnn.datavargan import dataset_to_datavar\n",
    "from pagnn.models.common import AdjacencyConv, SequenceConv, SequentialMod\n",
    "from pagnn.utils import expand_adjacency_tensor, padding_amount, reshape_internal_dim\n",
    "from pagnn.dataset import dataset_to_gan, row_to_dataset\n",
    "\n",
    "from kmtools import py_tools, sequence_tools\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from pagnn.types import DataRow, DataSetGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from '/home/kimlab1/database_data/datapkg/adjacency-net-v2/src/helper/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('validation_protherm_dataset')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEBOOK_PATH = Path('validation_protherm_dataset')\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/kimlab1/database_data/datapkg/adjacency-net-v2/notebooks/validation_protherm_dataset')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16ca70d'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc = subprocess.run([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], stdout=subprocess.PIPE)\n",
    "GIT_REV = proc.stdout.decode().strip()\n",
    "GIT_REV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "NETWORK_NAME = os.getenv(\"CI_COMMIT_SHA\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT, NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7b4ff1af3ec63a01fa415435420c554be1fecbb0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    NETWORK_NAME = \",\".join([\n",
    "        \"7b4ff1af3ec63a01fa415435420c554be1fecbb0\",  # test74\n",
    "    ])\n",
    "else:\n",
    "    assert NETWORK_NAME is not None\n",
    "    \n",
    "NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG[\"protherm_validaton_dataset\"] = Path(os.environ[\"DATAPKG_OUTPUT_DIR\"]).joinpath(\n",
    "    \"adjacency-net-v2\", \"v0.2\", \"protherm_dataset\", \"protherm_validaton_dataset.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/kimlab1/database_data/datapkg_output_dir/adjacency-net-v2/v0.2/protherm_dataset/protherm_validaton_dataset.parquet')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = DATAPKG[\"protherm_validaton_dataset\"].resolve(strict=True)\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_wt</th>\n",
       "      <th>chain_id</th>\n",
       "      <th>mutation</th>\n",
       "      <th>cartesian_ddg_beta_nov15_cart_1</th>\n",
       "      <th>ddg_exp</th>\n",
       "      <th>cartesian_ddg_beta_nov16_cart_1</th>\n",
       "      <th>cartesian_ddg_score12_cart_1</th>\n",
       "      <th>cartesian_ddg_talaris2013_cart_1</th>\n",
       "      <th>cartesian_ddg_talaris2014_cart_1</th>\n",
       "      <th>ddg_monomer_soft_rep_design_1</th>\n",
       "      <th>local_filename_wt</th>\n",
       "      <th>structure_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>qseq</th>\n",
       "      <th>residue_idx_1_corrected</th>\n",
       "      <th>residue_idx_2_corrected</th>\n",
       "      <th>distances</th>\n",
       "      <th>mutation_matches_sequence</th>\n",
       "      <th>qseq_mutation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/kimlab2/database_data/biological-data-wa...</td>\n",
       "      <td>A</td>\n",
       "      <td>G44S</td>\n",
       "      <td>-1.808667</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.289667</td>\n",
       "      <td>-0.633667</td>\n",
       "      <td>-2.384</td>\n",
       "      <td>/home/kimlab1/database_data/datapkg/adjacency-...</td>\n",
       "      <td>107l</td>\n",
       "      <td>0</td>\n",
       "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKGEL...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 66, 70, 91...</td>\n",
       "      <td>[1.3463744749991646, 4.7287436401778065, 6.389...</td>\n",
       "      <td>True</td>\n",
       "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/kimlab2/database_data/biological-data-wa...</td>\n",
       "      <td>A</td>\n",
       "      <td>A120M</td>\n",
       "      <td>2.617667</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.069000</td>\n",
       "      <td>-0.188000</td>\n",
       "      <td>2.472</td>\n",
       "      <td>/home/kimlab1/database_data/datapkg/adjacency-...</td>\n",
       "      <td>160l</td>\n",
       "      <td>0</td>\n",
       "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 66, 70, 91, 92...</td>\n",
       "      <td>[1.345760255620924, 4.727237880524551, 6.42311...</td>\n",
       "      <td>True</td>\n",
       "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         filename_wt chain_id mutation  \\\n",
       "0  /home/kimlab2/database_data/biological-data-wa...        A     G44S   \n",
       "1  /home/kimlab2/database_data/biological-data-wa...        A    A120M   \n",
       "\n",
       "   cartesian_ddg_beta_nov15_cart_1  ddg_exp  cartesian_ddg_beta_nov16_cart_1  \\\n",
       "0                        -1.808667    -0.53                           -0.701   \n",
       "1                         2.617667    -0.20                            0.354   \n",
       "\n",
       "   cartesian_ddg_score12_cart_1  cartesian_ddg_talaris2013_cart_1  \\\n",
       "0                         0.088                         -0.289667   \n",
       "1                         0.560                         -0.069000   \n",
       "\n",
       "   cartesian_ddg_talaris2014_cart_1  ddg_monomer_soft_rep_design_1  \\\n",
       "0                         -0.633667                         -2.384   \n",
       "1                         -0.188000                          2.472   \n",
       "\n",
       "                                   local_filename_wt structure_id  model_id  \\\n",
       "0  /home/kimlab1/database_data/datapkg/adjacency-...         107l         0   \n",
       "1  /home/kimlab1/database_data/datapkg/adjacency-...         160l         0   \n",
       "\n",
       "                                                qseq  \\\n",
       "0  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKGEL...   \n",
       "1  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
       "\n",
       "                             residue_idx_1_corrected  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                             residue_idx_2_corrected  \\\n",
       "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 66, 70, 91...   \n",
       "1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 66, 70, 91, 92...   \n",
       "\n",
       "                                           distances  \\\n",
       "0  [1.3463744749991646, 4.7287436401778065, 6.389...   \n",
       "1  [1.345760255620924, 4.727237880524551, 6.42311...   \n",
       "\n",
       "   mutation_matches_sequence  \\\n",
       "0                       True   \n",
       "1                       True   \n",
       "\n",
       "                                       qseq_mutation  \n",
       "0  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...  \n",
       "1  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pq.read_table(input_file).to_pandas()\n",
    "input_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.5913166222602743, pvalue=0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(input_df['cartesian_ddg_beta_nov16_cart_1'].values, input_df['ddg_exp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load master network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'network_state': PosixPath('/home/kimlab1/database_data/datapkg_output_dir/adjacency-net-v2/7b4ff1af3ec63a01fa415435420c554be1fecbb0/train_network/models/model_000000006753.state'),\n",
       " 'network_file': PosixPath('/home/kimlab1/database_data/datapkg_output_dir/adjacency-net-v2/7b4ff1af3ec63a01fa415435420c554be1fecbb0/train_network/model.py'),\n",
       " 'network_info': {'network_name': 'DCN_7b4ff1af3ec63a01fa415435420c554be1fecbb0',\n",
       "  'network_settings': {}},\n",
       " 'stats_db': PosixPath('/home/kimlab1/database_data/datapkg_output_dir/adjacency-net-v2/7b4ff1af3ec63a01fa415435420c554be1fecbb0/train_network/stats.db')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINED_NETWORKS[NETWORK_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCN_7b4ff1af3ec63a01fa415435420c554be1fecbb0(\n",
       "  (layer_1): SequentialMod(\n",
       "    (0): PairwiseConv(\n",
       "      (seq_cart_barcode_model): DistanceNet(\n",
       "        (linear1): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (linear2): Linear(in_features=64, out_features=12, bias=True)\n",
       "      )\n",
       "      (spatial_conv): Conv1d(26, 64, kernel_size=(2,), stride=(2,), bias=False)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer_n): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): RepeatPad()\n",
       "    (2): MaxPool1d(kernel_size=64, stride=64, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(128, 1, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_info = TRAINED_NETWORKS[NETWORK_NAME]['network_info']\n",
    "network_file = TRAINED_NETWORKS[NETWORK_NAME]['network_file']\n",
    "network_state = Path(TRAINED_NETWORKS[NETWORK_NAME]['network_state'])\n",
    "\n",
    "runpy.run_path(network_file)\n",
    "\n",
    "Net = getattr(pagnn.models.dcn, network_info[\"network_name\"])\n",
    "net = Net(**network_info[\"network_settings\"])\n",
    "net.load_state_dict(torch.load(network_state.as_posix()))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TL network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TupleToDataSet:\n",
    "    \n",
    "    def __init__(self, dataset_to_datavar):\n",
    "        self.dataset_to_datavar = dataset_to_datavar\n",
    "        \n",
    "    def __call__(self, tup):\n",
    "        # DataRow\n",
    "        row_pos = DataRow(\n",
    "            sequence=tup.sequence,\n",
    "            adjacency_idx_1=tup.adjacency_idx_1,\n",
    "            adjacency_idx_2=tup.adjacency_idx_2,\n",
    "            distances=tup.distances,\n",
    "            target=0\n",
    "        )\n",
    "        row_neg = DataRow(\n",
    "            sequence=tup.sequence_mut,\n",
    "            adjacency_idx_1=tup.adjacency_idx_1,\n",
    "            adjacency_idx_2=tup.adjacency_idx_2,\n",
    "            distances=tup.distances,\n",
    "            target=tup.ddg_exp,\n",
    "        )\n",
    "\n",
    "        # DataSet\n",
    "        permute_offset = pagnn.dataset.get_offset(len(tup.sequence.replace('-', '')), np.random.RandomState())\n",
    "        dataset_pos = dataset_to_gan(row_to_dataset(row_pos, permute_offset=permute_offset))\n",
    "        dataset_neg = dataset_to_gan(row_to_dataset(row_neg, permute_offset=permute_offset))\n",
    "\n",
    "        assert dataset_pos.adjs == dataset_neg.adjs\n",
    "        dataset = DataSetGAN(\n",
    "            dataset_pos.seqs + dataset_neg.seqs,\n",
    "            dataset_neg.adjs,\n",
    "            dataset_neg.meta,\n",
    "        )\n",
    "        \n",
    "        # DataVar\n",
    "        datavar = self.dataset_to_datavar(dataset)\n",
    "        return datavar, torch.tensor([tup.ddg_exp], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProthermData(Dataset):\n",
    "    \n",
    "    def __init__(self, input_file, transform) -> None:\n",
    "        input_df = pq.read_table(input_file).to_pandas()\n",
    "        input_df['sequence'] = input_df['qseq']\n",
    "        # input_df['sequence_mut'] = input_df.apply(mutate_sequence, axis=1)\n",
    "        input_df['sequence_mut'] = input_df['qseq_mutation']\n",
    "        input_df['adjacency_idx_1'] = input_df['residue_idx_1_corrected']\n",
    "        input_df['adjacency_idx_2'] = input_df['residue_idx_2_corrected']\n",
    "        \n",
    "        columns = [\"sequence\", \"sequence_mut\", \"adjacency_idx_1\", \"adjacency_idx_2\", \"distances\", \"ddg_exp\"]\n",
    "        self.tuples = list(input_df[columns].itertuples())\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tuples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tup = self.tuples[index]\n",
    "        datapoint = self.transform(tup)\n",
    "        return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProthermTransferLearner(nn.Module):\n",
    "    \n",
    "    def __init__(self, master_model) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.master_model = copy.deepcopy(master_model)      \n",
    "        self.master_model.eval()\n",
    "\n",
    "        for param in self.master_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.master_model.layer_n.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "#         for param in self.master_model.layer_n[3].parameters():\n",
    "#             param.requires_grad = True\n",
    "        \n",
    "        input_size = self.master_model.hidden_size\n",
    "        if False:\n",
    "            input_size *= 2  # For wt and mut\n",
    "            hidden_size = input_size * 2\n",
    "        else:\n",
    "            hidden_size = input_size\n",
    "        \n",
    "\n",
    "        self.layer_n = nn.Sequential(\n",
    "#             nn.Conv1d(\n",
    "#                 input_size,\n",
    "#                 hidden_size,\n",
    "#                 kernel_size=self.master_model.kernel_size,\n",
    "#                 stride=self.master_model.stride,\n",
    "#                 padding=self.master_model.padding,\n",
    "#                 bias=True,\n",
    "#             ),\n",
    "            nn.MaxPool1d(4000),\n",
    "            nn.Conv1d(hidden_size, 1, kernel_size=1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, adjs):\n",
    "        x_wt = seq[0:1]\n",
    "        x_wt = self.master_model.layer_1(x_wt, adjs[0][0])\n",
    "\n",
    "        x_mut = seq[1:2]\n",
    "        x_mut = self.master_model.layer_1(x_mut, adjs[0][0])\n",
    "\n",
    "        if False:\n",
    "            x = torch.cat([x_wt, x_mut], dim=1)\n",
    "            x = self.layer_n(x)\n",
    "        elif False:\n",
    "            x_wt = self.master_model.layer_n(x_wt)\n",
    "            x_mut = self.master_model.layer_n(x_mut)\n",
    "            x = (x_wt - x_mut).sum()\n",
    "        else:\n",
    "            x = x_wt - x_mut\n",
    "            x = self.master_model.layer_n(x)\n",
    "            x = x.sum()\n",
    "\n",
    "        # Layer N\n",
    "        return x\n",
    "    \n",
    "    def dataset_to_datavar(self, *args, **kwargs):\n",
    "        return self.master_model.dataset_to_datavar(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl = ProthermTransferLearner(net).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProthermData(\n",
    "    input_file,\n",
    "    transform=transforms.Compose([\n",
    "        TupleToDataSet(net_tl.dataset_to_datavar),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3797, device='cuda:0', grad_fn=<SqueezeBackward0>) tensor([-0.5300])\n"
     ]
    }
   ],
   "source": [
    "dv, target = dataset[0]\n",
    "\n",
    "for dv, target in dataset:\n",
    "    out = net_tl(dv.seqs.cuda(), [[dv.adjs[0].cuda()]])\n",
    "    print(out.squeeze(), target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = torch.utils.data.random_split(\n",
    "    dataset, [int(len(dataset) * 0.70), len(dataset) - int(len(dataset) * 0.70)]\n",
    ")\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(dataset_train),\n",
    "    'val': len(dataset_val),\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(dataset_train, batch_size=64, shuffle=False, num_workers=0, collate_fn=list),\n",
    "    \"val\": DataLoader(dataset_train, batch_size=1, shuffle=False, num_workers=0, collate_fn=list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            #             if phase == 'train':\n",
    "            #                 if scheduler is not None:\n",
    "            #                     scheduler.step()\n",
    "            #                 model.train()  # Set model to training mode\n",
    "            #             else:\n",
    "            #                 model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            pred_list = []\n",
    "            target_list = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, batch in enumerate(dataloaders[phase]):\n",
    "                for j, (dv, target) in enumerate(batch):\n",
    "                    dv = dv._replace(seqs=dv.seqs.cuda(), adjs=[dv.adjs[0].cuda()])\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        preds = model(dv.seqs, [dv.adjs])\n",
    "                        if False:\n",
    "                            preds_diff = preds[0] - preds[1]\n",
    "                            preds_diff_sum = preds_diff.sum().squeeze()\n",
    "                        else:\n",
    "                            preds_diff_sum = preds.squeeze()\n",
    "\n",
    "                        loss = criterion(preds_diff_sum, target)\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    running_corrects += torch.mean(torch.abs(preds_diff_sum - target.data))\n",
    "                    pred_list.append(preds_diff_sum.cpu().data.numpy())\n",
    "                    target_list.append(target.cpu().data.numpy())\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            pearson_corr = stats.pearsonr(np.hstack(pred_list), np.hstack(target_list))[0]\n",
    "            spearman_corr = stats.spearmanr(np.hstack(pred_list), np.hstack(target_list))[0]\n",
    "\n",
    "            print(\n",
    "                f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} \"\n",
    "                f\"Pearson {pearson_corr:.4f} Spearman {spearman_corr:.4f}\"\n",
    "            )\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training complete in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl = ProthermTransferLearner(net)\n",
    "net_tl = net_tl.to(device=\"cuda\")\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(net_tl.master_model.layer_n.parameters(), lr=0.001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 3.1717 Acc: 3.1717 Pearson 0.0957 Spearman 0.0739\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-918af24c492e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_tl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-a4c2dbf4f28e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adjacency-net-v2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adjacency-net-v2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adjacency-net-v2/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-432b6cef7020>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdatapoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adjacency-net-v2/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-284ef2bed8f3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# DataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpermute_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpagnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdataset_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_to_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermute_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdataset_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_to_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermute_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working/pagnn/pagnn/dataset.py\u001b[0m in \u001b[0;36mrow_to_dataset\u001b[0;34m(row, target, permute_offset)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mDataSet\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjacency_idx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjacency_idx_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working/pagnn/pagnn/utils/dataset_ops.py\u001b[0m in \u001b[0;36mseq_to_array\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mseq_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_coo_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# import pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = train_model(net_tl, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset[4]\n",
    "net_tl(ds.seqs, [ds.adjs]).mean(2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset:\n",
    "    print(net_tl(ds.seqs, [ds.adjs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_neg.adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_pos.adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try transformations like shuffle by same amount\n",
    "dataloader = DataLoader(dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_df[['sequence', 'sequence_mut', 'adjacency_idx_1', 'adjacency_idx_2', 'distances', 'ddg_exp']].copy()\n",
    "\n",
    "from pagnn.dataset import dataset_to_gan, row_to_dataset\n",
    "\n",
    "for row in df.itertuples():\n",
    "    row_pos = DataRow\n",
    "    dataset = dataset_to_gan(row_to_dataset(row, 0))\n",
    "    datavar = net_tl.dataset_to_datavar(dataset)\n",
    "    outputs = net_tl(datavar.seqs, [datavar.adjs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(row):\n",
    "    sequence = row['sequence']\n",
    "    wt = row['mutation'][0]\n",
    "    pos = int(row['mutation'][1:-1])\n",
    "    mut = row['mutation'][-1]\n",
    "    sequence_mut = sequence[:pos - 1] + mut + sequence[pos:]\n",
    "    assert len(sequence) == len(sequence_mut)\n",
    "    return sequence_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['sequence'] = input_df['qseq']\n",
    "input_df['sequence_mut'] = input_df.apply(mutate_sequence, axis=1)\n",
    "# input_df['sequence_mut'] = input_df['qseq_mutation']\n",
    "input_df['adjacency_idx_1'] = input_df['residue_idx_1_corrected']\n",
    "input_df['adjacency_idx_2'] = input_df['residue_idx_2_corrected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    input_df[f'{network_name}_wt'] = helper.predict_with_network(\n",
    "        input_df[['sequence', 'adjacency_idx_1', 'adjacency_idx_2', 'distances']]\n",
    "            .copy(),\n",
    "        network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "        network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "    )\n",
    "    input_df[f'{network_name}_mut'] = helper.predict_with_network(\n",
    "        input_df[['sequence_mut', 'adjacency_idx_1', 'adjacency_idx_2', 'distances']]\n",
    "            .rename(columns={'sequence_mut': 'sequence'}).copy(),\n",
    "        network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "        network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    input_df[f'{network_name}_change'] = (\n",
    "        input_df[f'{network_name}_mut'] -\n",
    "        input_df[f'{network_name}_wt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(input_df, preserve_index=True)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    OUTPUT_PATH.joinpath(\"validation_protherm_dataset.parquet\"),\n",
    "    version='2.0',\n",
    "    flavor='spark',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "424px",
    "left": "26px",
    "top": "141px",
    "width": "187px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
