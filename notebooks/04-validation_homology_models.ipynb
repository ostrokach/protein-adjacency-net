{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import itertools\n",
    "import importlib\n",
    "import multiprocessing\n",
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "from scipy import stats\n",
    "\n",
    "from kmtools import py_tools, sequence_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "    \n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path('validation_homology_models')\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = subprocess.run([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], stdout=subprocess.PIPE)\n",
    "GIT_REV = proc.stdout.decode().strip()\n",
    "GIT_REV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "NETWORK_NAME = os.getenv(\"CI_COMMIT_SHA\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT, NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    NETWORK_NAME = \"dcn_old_0,6bbf5b792c30570b8ab1a4c1b3426cdc6ad84446\"\n",
    "else:\n",
    "    assert NETWORK_NAME is not None\n",
    "    \n",
    "NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     %load_ext autoreload\n",
    "#     %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['uniparc-domain-wstructure'] = (\n",
    "    Path(os.environ['DATAPKG_OUTPUT_DIR'])\n",
    "    .joinpath(\"uniparc-domain-wstructure\", \"master\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['adjacency_net_v2'] = (\n",
    "    Path(os.environ['DATAPKG_OUTPUT_DIR'])\n",
    "    .joinpath(\"adjacency_net_v2\", \"master\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['hhsuite-wstructure'] = {\n",
    "    'pdb_homology_modeling': \n",
    "        Path(os.environ['DATAPKG_OUTPUT_DIR']).joinpath(\n",
    "            \"hhsuite-wstructure\",\n",
    "            \"master\",\n",
    "            \"pdb_homology_modeling\",\n",
    "        ),\n",
    "    'pdb_homology_modeling_adjacency_matrix': \n",
    "        Path(os.environ['DATAPKG_OUTPUT_DIR']).joinpath(\n",
    "            \"hhsuite-wstructure\",\n",
    "#             \"master\",\n",
    "#             \"354abf6aa8a49dded9955be5580bde4d6ac10c60\",\n",
    "            \"e062231f4490081db273d21ec832acb18f36bcbb\",\n",
    "            \"pdb_homology_modeling_adjacency_matrix\",\n",
    "        ),\n",
    "    'pdb_homology_modeling_rosetta_score':  \n",
    "        Path(os.environ['DATAPKG_OUTPUT_DIR']).joinpath(\n",
    "            \"hhsuite-wstructure\",\n",
    "#             \"master\",\n",
    "            \"09ae1a7157c6c98e8b7bdd93b0e7e6d4cf7587fc\",\n",
    "            \"pdb_homology_modeling_rosetta_score\",\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pdb_homology_modeling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_model_files = (\n",
    "    sorted(DATAPKG['hhsuite-wstructure']['pdb_homology_modeling'].glob('*-?.parquet')) +\n",
    "    sorted(DATAPKG['hhsuite-wstructure']['pdb_homology_modeling'].glob('*-??.parquet'))    \n",
    ")\n",
    "\n",
    "assert len(homology_model_files) == 10\n",
    "homology_model_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"unique_id\", \"query_id\",\n",
    "\n",
    "    \"dope_score\", \"ga341_score\", \"dope_score_norm\", \"molpdf\",\n",
    "    \n",
    "    \"identity_calc\", \"coverage_calc\",\n",
    "\n",
    "    \"score\", \"similarity\", \"sum_probs\",\n",
    "    \"evalue\", \"probability\", \"identity\",\n",
    "\n",
    "    \"query_ali\", \"template_ali\",\n",
    "    \"query_match_length\", \"template_match_length\",\n",
    "    \n",
    "    \"sequence\",\n",
    "    'hm_residue_idx_1', 'hm_residue_idx_2',\n",
    "    \"adjacency_idx_1_from_pdb\", \"adjacency_idx_2_from_pdb\", \n",
    "    \"adjacency_idx_1_from_hm\", \"adjacency_idx_2_from_hm\",\n",
    "]\n",
    "\n",
    "pdb_homology_modeling = (\n",
    "    pq.ParquetDataset(homology_model_files)\n",
    "    .read_pandas(columns=columns)\n",
    "    .to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pdb_homology_modeling.head(1))\n",
    "print(len(pdb_homology_modeling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `pdb_homology_modeling_adjacency_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_homology_modeling_adjacency_matrix_files = sorted(\n",
    "    DATAPKG\n",
    "    ['hhsuite-wstructure']\n",
    "    ['pdb_homology_modeling_adjacency_matrix']\n",
    "    .glob('*-???.parquet')\n",
    ")\n",
    "\n",
    "assert len(pdb_homology_modeling_adjacency_matrix_files) == 10\n",
    "pdb_homology_modeling_adjacency_matrix_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in pdb_homology_modeling_adjacency_matrix_files:\n",
    "    df = pq.read_table(file, columns=columns, use_pandas_metadata=True).to_pandas()\n",
    "    dfs.append(df)\n",
    "    \n",
    "\n",
    "pdb_homology_modeling_adjacency_matrix = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb_homology_modeling_adjacency_matrix = (\n",
    "#     pq.ParquetDataset(pdb_homology_modeling_adjacency_matrix_files)\n",
    "#     .read_pandas()\n",
    "#     .to_pandas()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(pdb_homology_modeling_adjacency_matrix.head(1))\n",
    "print(len(pdb_homology_modeling_adjacency_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pdb_homology_modeling_rosetta_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(\n",
    "    DATAPKG\n",
    "    ['hhsuite-wstructure']\n",
    "    ['pdb_homology_modeling_rosetta_score']\n",
    "    .glob('*-???-?.parquet')\n",
    ")\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_homology_modeling_rosetta_score = (\n",
    "    pq.ParquetDataset(files)\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(pdb_homology_modeling_rosetta_score.head(1))\n",
    "print(len(pdb_homology_modeling_rosetta_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine into a single dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `homology_models_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_models_dataset = (\n",
    "    pdb_homology_modeling\n",
    "    .merge(pdb_homology_modeling_adjacency_matrix, left_index=True, right_index=True, suffixes=(\"\", \"_2\"))\n",
    "    .merge(pdb_homology_modeling_rosetta_score, left_index=True, right_index=True, suffixes=(\"\", \"_3\"))\n",
    ")\n",
    "\n",
    "assert (homology_models_dataset['unique_id'] == homology_models_dataset['unique_id_2']).all()\n",
    "del homology_models_dataset['unique_id_2']\n",
    "\n",
    "assert (homology_models_dataset['unique_id'] == homology_models_dataset['unique_id_3']).all()\n",
    "del homology_models_dataset['unique_id_3']\n",
    "\n",
    "# assert len(homology_models_dataset) == len(pdb_homology_modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_models_dataset['sequence'] = homology_models_dataset['query_ali'].str.replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(homology_models_dataset.head(1))\n",
    "print(len(homology_models_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id, group in homology_models_dataset.groupby('query_id'):\n",
    "    assert (group['sequence'].str.replace('-', '') == group['sequence'].iloc[0].replace('-', '')).all()\n",
    "    assert (group['query_match_length'] == group['query_match_length'].iloc[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(homology_models_dataset.head(2))\n",
    "print(len(homology_models_dataset))\n",
    "print(len(set(homology_models_dataset['query_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(col, range, *args, **kwargs):\n",
    "    x = homology_models_dataset[col]\n",
    "    x = np.clip(x, *range)\n",
    "    with plt.rc_context(rc={\"font.size\": 12}):\n",
    "        plt.hist(x, range=range, **kwargs)\n",
    "        plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, axs = plt.subplots(3, 2, figsize=(8, 8))\n",
    "\n",
    "plt.sca(axs[0, 0])\n",
    "hist(\"identity_calc\", range=(0, 1), bins=30)\n",
    "\n",
    "plt.sca(axs[0, 1])\n",
    "hist(\"coverage_calc\", range=(0.6, 1), bins=30)\n",
    "\n",
    "plt.sca(axs[1, 0])\n",
    "hist(\"score\", range=(0, 500), bins=30)\n",
    "\n",
    "plt.sca(axs[1, 1])\n",
    "hist(\"sum_probs\", range=(0, 200), bins=30)\n",
    "\n",
    "plt.sca(axs[2, 0])\n",
    "hist(\"query_match_length\", range=(0, 300), bins=30)\n",
    "\n",
    "plt.sca(axs[2, 1])\n",
    "hist(\"template_match_length\", range=(0, 300), bins=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `homology_models_dataset_final`\n",
    "\n",
    "We need to cover a reasonable amount of sequence with an adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_models_dataset[\"frac_aa_wadj_pdb\"] = (\n",
    "    homology_models_dataset[\"adjacency_idx_1_from_pdb\"].apply(lambda l: len(set(l))) /\n",
    "    homology_models_dataset['sequence'].str.len()\n",
    ")\n",
    "\n",
    "homology_models_dataset[\"frac_aa_wadj_hm\"] = (\n",
    "    homology_models_dataset[\"hm_residue_idx_1\"].apply(lambda l: len(set(l))) /\n",
    "    homology_models_dataset['sequence'].str.len()\n",
    ")\n",
    "\n",
    "homology_models_dataset[\"frac_aa_wadj_hm2\"] = (\n",
    "    homology_models_dataset[\"adjacency_idx_1_from_hm\"].apply(lambda l: len(set(l))) /\n",
    "    homology_models_dataset['sequence'].str.len()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_models_dataset_final = (\n",
    "    homology_models_dataset[\n",
    "        (homology_models_dataset['frac_aa_wadj_pdb'] > 0.6) &\n",
    "        (homology_models_dataset['frac_aa_wadj_hm'] > 0.6)\n",
    "    ]\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, axs = plt.subplots(3, 2, figsize=(6, 6))\n",
    "\n",
    "for i, col in enumerate(['frac_aa_wadj_pdb', 'frac_aa_wadj_hm', 'frac_aa_wadj_hm2']):\n",
    "    range_ = (0, 1)\n",
    "    \n",
    "    plt.sca(axs[i, 0])\n",
    "    vals = np.clip(homology_models_dataset[col], *range_)\n",
    "    plt.hist(vals)\n",
    "    plt.xlabel(col)\n",
    "    if i == 0:\n",
    "        plt.title(\"Raw\")\n",
    "\n",
    "    plt.sca(axs[i, 1])\n",
    "    vals = np.clip(homology_models_dataset_final[col], *range_)\n",
    "    plt.hist(vals)\n",
    "    plt.xlabel(col)\n",
    "    if i == 0:\n",
    "        plt.title(\"Filtered\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    homology_models_dataset_final = homology_models_dataset_final.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions using PDB adjacencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for network_name in reversed(NETWORK_NAME.split(',')):\n",
    "    homology_models_dataset_final[f'{network_name[:7]}_pdb'] = (\n",
    "        helper.predict_with_network(\n",
    "            homology_models_dataset_final.rename(columns={\n",
    "                'adjacency_idx_1_from_pdb': 'adjacency_idx_1',\n",
    "                'adjacency_idx_2_from_pdb': 'adjacency_idx_2',\n",
    "            }),\n",
    "            network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "            network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "        )\n",
    "    )\n",
    "    assert homology_models_dataset_final[f'{network_name[:7]}_pdb'].notnull().all(), network_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions using HM adjacencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    homology_models_dataset_final[f'{network_name[:7]}_hm'] = (\n",
    "        helper.predict_with_network(\n",
    "            homology_models_dataset_final.rename(columns={\n",
    "                'hm_residue_idx_1': 'adjacency_idx_1',\n",
    "                'hm_residue_idx_2': 'adjacency_idx_2',\n",
    "            }),\n",
    "            network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "            network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "        )\n",
    "    )\n",
    "    assert homology_models_dataset_final[f'{network_name[:7]}_hm'].notnull().all(), network_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(homology_models_dataset_final, preserve_index=True)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    OUTPUT_PATH.joinpath(f\"{NETWORK_NAME}_dataset.parquet\"),\n",
    "    version='2.0',\n",
    "    flavor='spark',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, columns):\n",
    "    \n",
    "    mat = np.zeros((len(columns), len(columns)), float)\n",
    "    for i, c1 in enumerate(columns):\n",
    "        for j, c2 in enumerate(columns):\n",
    "            mat[i, j] = stats.spearmanr(df[c1], df[c2])[0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(mat)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(columns)))\n",
    "    ax.set_yticks(np.arange(len(columns)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(columns)\n",
    "    ax.set_yticklabels(columns)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(len(columns)):\n",
    "            text = ax.text(j, i, f\"{mat[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Spearman correlation between alignment, structure, and network scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'dope_score',\n",
    "    'dope_score_norm',\n",
    "    'ga341_score',\n",
    "    'rosetta_score',\n",
    "]\n",
    "\n",
    "feature_columns = [\n",
    "    \"identity_calc\",\n",
    "    # \"coverage_calc\", \n",
    "\n",
    "    \"identity\", \n",
    "    \"similarity\",\n",
    "    \"score\",  # \"probability\", \"evalue\",\n",
    "    \"sum_probs\",\n",
    "    \n",
    "    \"query_match_length\", \n",
    "    \"template_match_length\",\n",
    "]\n",
    "\n",
    "network_columns = [\n",
    "    f\"{network_name[:7]}{suffix}\"\n",
    "    for network_name in NETWORK_NAME.split(',')\n",
    "    for suffix in [\n",
    "        \"_pdb\",\n",
    "        \"_hm\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "results_df = homology_models_dataset_final.dropna(subset=network_columns).copy()\n",
    "print(f\"Lost {len(homology_models_dataset_final) - len(results_df)} columns with nulls!\")\n",
    "\n",
    "for col in ['dope_score', 'dope_score_norm', 'rosetta_score']:\n",
    "    results_df[col] = -results_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with plt.rc_context(rc={'figure.figsize': (10, 10), 'font.size': 11}):\n",
    "    plot(results_df, target_columns + feature_columns + network_columns)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH.joinpath(f\"{NETWORK_NAME}_corr_all.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUTPUT_PATH.joinpath(f\"{NETWORK_NAME}_corr_all.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for query_id, group in results_df.groupby('query_id'):\n",
    "    assert (group['sequence'].str.replace('-', '') == group['sequence'].iloc[0].replace('-', '')).all()\n",
    "    assert (group['query_match_length'] == group['query_match_length'].iloc[0]).all()\n",
    "\n",
    "    if len(group) < 3:\n",
    "        print(f\"Skipping small group for query_id = '{query_id}'\")\n",
    "        continue\n",
    "\n",
    "    for y_col in target_columns:\n",
    "        if len(group) < 3 or len(set(group[y_col])) == 1:\n",
    "            print(f\"skipping y_col '{y_col}'\")\n",
    "            continue\n",
    "        for x_col in feature_columns + network_columns:\n",
    "            if x_col in ['query_match_length']:\n",
    "                continue\n",
    "            if len(group) < 3 or len(set(group[x_col])) == 1:\n",
    "                print(f\"skipping x_col '{x_col}'\")\n",
    "                continue\n",
    "            corr, pvalue = stats.spearmanr(group[x_col], group[y_col])\n",
    "            data.append((y_col, x_col, corr, pvalue))\n",
    "            \n",
    "correlations_df = pd.DataFrame(data, columns=['target', 'feature', 'correlation', 'pvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ignore = ['query_match_length']\n",
    "\n",
    "with plt.rc_context(rc={'figure.figsize': (12, 8), 'font.size': 12}):\n",
    "    fg, axs = plt.subplots(2, 2)\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        plt.sca(axs[i // 2, i % 2])\n",
    "\n",
    "        plt.title(target)\n",
    "\n",
    "        xticks = [c for c in feature_columns + network_columns if c not in ignore]\n",
    "\n",
    "        corr = [\n",
    "            correlations_df[\n",
    "                (correlations_df['target'] == target) &\n",
    "                (correlations_df['feature'] == feature)\n",
    "            ]['correlation'].values\n",
    "            for feature in xticks\n",
    "        ]\n",
    "\n",
    "        plt.boxplot(corr)\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "\n",
    "        if i // 2:\n",
    "            plt.xticks(range(1, len(xticks) + 1), xticks, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        else:\n",
    "            plt.xticks(range(1, len(xticks) + 1), [None for _ in xticks])\n",
    "            \n",
    "        if i % 2:\n",
    "#             plt.yticks(plt.yticks()[0], [None for _ in plt.yticks()[0]])\n",
    "            pass\n",
    "        else:\n",
    "            plt.ylabel(\"Spearman R\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH.joinpath(f\"{NETWORK_NAME}_corr_gby_query.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUTPUT_PATH.joinpath(f\"{NETWORK_NAME}_corr_gby_query.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "556px",
    "left": "25px",
    "top": "134px",
    "width": "248px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
