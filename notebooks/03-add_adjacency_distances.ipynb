{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import concurrent.futures.process\n",
    "import importlib\n",
    "import os\n",
    "import shlex\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import yaml\n",
    "\n",
    "from kmbio import PDB\n",
    "from kmtools import structure_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path(os.getenv(\"CI_JOB_NAME\", \"add_adjacency_distances\"))\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd().expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "ADJACENCY_MATRIX_PARQUET_PATH = os.getenv(\"ADJACENCY_MATRIX_PARQUET_PATH\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "ADJACENCY_MATRIX_PARQUET_PATH = (\n",
    "    Path(ADJACENCY_MATRIX_PARQUET_PATH).expanduser()\n",
    "    if ADJACENCY_MATRIX_PARQUET_PATH is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "TASK_ID, TASK_COUNT, ADJACENCY_MATRIX_PARQUET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "\n",
    "if DEBUG:\n",
    "    TASK_ID = 78\n",
    "    TASK_COUNT = 1029\n",
    "    ADJACENCY_MATRIX_PARQUET_PATH = (\n",
    "        Path(os.getenv(\"DATAPKG_OUTPUT_DIR\"))\n",
    "        .joinpath(\"adjacency-net-v2\", \"master\", \"training_dataset\", \"adjacency_matrix.parquet\")\n",
    "    )\n",
    "else:\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "    assert ADJACENCY_MATRIX_PARQUET_PATH is not None\n",
    "\n",
    "assert ADJACENCY_MATRIX_PARQUET_PATH.is_dir()\n",
    "\n",
    "TASK_ID, TASK_COUNT, ADJACENCY_MATRIX_PARQUET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['pdb-ffindex'] = {\n",
    "    'pdb_mmcif_ffindex': (\n",
    "        Path(os.environ['DATAPKG_OUTPUT_DIR'])\n",
    "        .joinpath(\"pdb-ffindex\", \"master\", \"pdb_mmcif_ffindex\", \"pdb-mmcif\")\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([\n",
    "    f for f in ADJACENCY_MATRIX_PARQUET_PATH.glob(\"**/*.parquet\")\n",
    "    if f.is_file()\n",
    "])\n",
    "\n",
    "print(files[:2])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{f.parent.parent for f in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = int(np.ceil(len(files) / TASK_COUNT))\n",
    "if len(files) > chunk_size:\n",
    "    files = files[(TASK_ID - 1) * chunk_size:TASK_ID * chunk_size]\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pq.ParquetFile(files[0])\n",
    "    .read_row_group(0, use_pandas_metadata=True)\n",
    "    .to_pandas(integer_object_nulls=True)\n",
    "    .set_index(\"__index_level_0__\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(data):\n",
    "    row = helper.to_namedtuple(data)\n",
    "    results = {}\n",
    "    try:\n",
    "        results['residue_idx_1'], results['residue_idx_2'], results['distances'] = (\n",
    "            helper.get_adjacency_with_distances(\n",
    "                row, max_cutoff=12, min_cutoff=None, structure_url_prefix=STRUCTURE_URL_PREFIX\n",
    "            )\n",
    "        )\n",
    "#         results['residue_idx_1'] = results['residue_idx_1'].tolist()\n",
    "#         results['residue_idx_2'] = results['residue_idx_2'].tolist()\n",
    "#         results['distances'] = results['distances'].tolist()\n",
    "        results[\"error\"] = None\n",
    "    except Exception as e:\n",
    "        results[\"error\"] = f\"{type(e)}: {e}\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run worker for single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(islice(df.itertuples(), 3))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRUCTURE_URL_PREFIX = f\"ff://{DATAPKG['pdb-ffindex']['pdb_mmcif_ffindex']}?\"\n",
    "STRUCTURE_URL_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "residue_idx_1, residue_idx_2, distance = helper.get_adjacency_with_distances(\n",
    "    row, max_cutoff=12, min_cutoff=None, structure_url_prefix=STRUCTURE_URL_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots()\n",
    "ax.hist(distance, range=(0, 12), bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker(row._asdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file(file):\n",
    "    file_parts = list(file.parts)\n",
    "    file_parts[-4] = file_parts[-4] + \"_wdistances\"\n",
    "    new_file = Path(*file_parts)\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    ds = pq.ParquetFile(file)\n",
    "    for row_group in range(ds.num_row_groups):\n",
    "        df = (\n",
    "            ds.read_row_group(row_group, use_pandas_metadata=True)\n",
    "            .to_pandas(integer_object_nulls=True)\n",
    "            .set_index(\"__index_level_0__\")\n",
    "        )\n",
    "        try:\n",
    "            with concurrent.futures.ProcessPoolExecutor(psutil.cpu_count(logical=False)) as pool:\n",
    "                futures = pool.map(\n",
    "                    worker,\n",
    "                    (t._asdict() for t in df[helper.GET_ADJACENCY_WITH_DISTANCES_ROW_ATTRIBUTES].itertuples()),\n",
    "                    chunksize=1)\n",
    "                results = list(futures)\n",
    "                results_df = pd.DataFrame(results)\n",
    "                df[\"residue_idx_1_corrected\"] = results_df[\"residue_idx_1\"].values\n",
    "                df[\"residue_idx_2_corrected\"] = results_df[\"residue_idx_2\"].values\n",
    "                df[\"distances\"] = results_df[\"distances\"].values\n",
    "                df[\"error_adding_distances\"] = results_df[\"error\"].values\n",
    "                num_errors = df[\"error_adding_distances\"].notnull().sum()\n",
    "                if num_errors:\n",
    "                    print(f\"Encountered {num_errors} errors when parsing file '{file}'.\")\n",
    "        except concurrent.futures.process.BrokenProcessPool as e:\n",
    "            warnings.warn(\n",
    "                f\"ProcessPool crashed while processing row_group '{row_group}' in file '{file}'.\"\n",
    "                f\"The error is '{type(e)}': {e}.\"\n",
    "            )\n",
    "            break\n",
    "        df = df.dropna(subset=[\"residue_idx_1_corrected\", \"residue_idx_2_corrected\", \"distances\"])\n",
    "        table = pa.Table.from_pandas(df, preserve_index=True)\n",
    "        if row_group == 0:\n",
    "            new_file = get_new_file(file)\n",
    "            new_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            writer = pq.ParquetWriter(new_file, table.schema, version=\"2.0\", flavor=\"spark\")\n",
    "        writer.write_table(table)\n",
    "        if row_group == ds.num_row_groups - 1:\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    display(df.head())\n",
    "\n",
    "    # Make sure that the file we wrote makes sense\n",
    "    for file in files:\n",
    "        new_file = get_new_file(file)\n",
    "\n",
    "        ds = pq.ParquetFile(file)\n",
    "        ds_new = pq.ParquetFile(new_file)\n",
    "        assert ds.num_row_groups == ds_new.num_row_groups\n",
    "\n",
    "        for row_group in range(ds.num_row_groups):\n",
    "            df = (\n",
    "                ds.read_row_group(0, use_pandas_metadata=True)\n",
    "                .to_pandas(integer_object_nulls=True)\n",
    "                .set_index(\"__index_level_0__\")\n",
    "            )\n",
    "            df_new = (\n",
    "                ds_new.read_row_group(0, use_pandas_metadata=True)\n",
    "                .to_pandas(integer_object_nulls=True)\n",
    "#                 .set_index(\"__index_level_0__\")\n",
    "            )\n",
    "            shared_columns = [\n",
    "                c for c in df.columns\n",
    "                if c in df_new.columns\n",
    "                and c not in [\n",
    "                    'a2b', 'b2a', 'residue_idx_1', 'residue_idx_2',\n",
    "                    'residue_id_1', 'residue_id_2', 'residue_aa_1', 'residue_aa_2',\n",
    "                    \"residue_idx_1_corrected\", \"residue_idx_2_corrected\"]\n",
    "            ]\n",
    "            assert (df[shared_columns] == df_new[shared_columns]).all().all()\n",
    "            assert all(\n",
    "                (l1 == l2).all() \n",
    "                for l1, l2\n",
    "                in zip(df[\"residue_id_1\"].values, df_new[\"residue_id_1\"].values)\n",
    "            )\n",
    "            assert all(\n",
    "                (l1.shape != l2.shape or not (l1 == l2).all())\n",
    "                for l1, l2\n",
    "                in zip(df[\"residue_idx_1_corrected\"].values, df_new[\"residue_idx_1_corrected\"].values)\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482px",
    "left": "34.9826px",
    "top": "126.979px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
