{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import itertools\n",
    "import importlib\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numba import njit, prange\n",
    "from scipy import stats\n",
    "\n",
    "import pagnn.models.dcn\n",
    "from pagnn.datavargan import dataset_to_datavar\n",
    "from pagnn.models.common import AdjacencyConv, SequenceConv, SequentialMod\n",
    "from pagnn.utils import expand_adjacency_tensor, padding_amount, reshape_internal_dim\n",
    "from pagnn.dataset import dataset_to_gan, row_to_dataset\n",
    "\n",
    "from kmtools import py_tools, sequence_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path('validation_protherm_dataset')\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = subprocess.run([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], stdout=subprocess.PIPE)\n",
    "GIT_REV = proc.stdout.decode().strip()\n",
    "GIT_REV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "NETWORK_NAME = os.getenv(\"CI_COMMIT_SHA\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT, NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    NETWORK_NAME = \",\".join([\n",
    "        \"7b4ff1af3ec63a01fa415435420c554be1fecbb0\",  # test74\n",
    "    ])\n",
    "else:\n",
    "    assert NETWORK_NAME is not None\n",
    "    \n",
    "NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['protherm_validaton_dataset'] = (\n",
    "    Path(os.environ['DATAPKG_OUTPUT_DIR'])\n",
    "    .joinpath(\"adjacency-net-v2\", \"v0.2\", \"protherm_dataset\", \"protherm_validaton_dataset.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = (\n",
    "    DATAPKG['protherm_validaton_dataset']\n",
    "    .resolve(strict=True)\n",
    ")\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pq.read_table(input_file).to_pandas()\n",
    "input_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(input_df['cartesian_ddg_beta_nov16_cart_1'].values, input_df['ddg_exp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_NETWORKS[NETWORK_NAME]['network_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_NETWORKS[NETWORK_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_info = TRAINED_NETWORKS[NETWORK_NAME]['network_info']\n",
    "network_file = TRAINED_NETWORKS[NETWORK_NAME]['network_file']\n",
    "network_state = Path(TRAINED_NETWORKS[NETWORK_NAME]['network_state'])\n",
    "\n",
    "runpy.run_path(network_file)\n",
    "\n",
    "Net = getattr(pagnn.models.dcn, network_info[\"network_name\"])\n",
    "net = Net(**network_info[\"network_settings\"])\n",
    "net.load_state_dict(torch.load(network_state.as_posix()))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProthermTransferLearner(nn.Module):\n",
    "    \n",
    "    def __init__(self, master_model) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.master_model = master_model      \n",
    "        self.master_model.eval()\n",
    "\n",
    "        for param in self.master_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.master_model.layer_n.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, seq, adjs):\n",
    "        return self.master_model.forward(seq, adjs)\n",
    "    \n",
    "    def dataset_to_datavar(self, *args, **kwargs):\n",
    "        return self.master_model.dataset_to_datavar(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl = ProthermTransferLearner(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from pagnn.types import DataRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        offset = np.random.randint(3, len(row_pos.sequence) - 3)\n",
    "\n",
    "        datavar_pos = \n",
    "        datavar_neg = \n",
    "            dataset = dataset_to_gan(row_to_dataset(row, 0))\n",
    "    datavar = net_tl.dataset_to_datavar(dataset)\n",
    "    outputs = net_tl(datavar.seqs, [datavar.adjs])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    dataset_to_datavar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TupleToDataSet:\n",
    "    \n",
    "    def __init__(self, dataset_to_datavar):\n",
    "        self.dataset_to_datavar = dataset_to_datavar\n",
    "        \n",
    "    def __call__(self, tup):\n",
    "        # DataRow\n",
    "        row_pos = DataRow(\n",
    "            sequence=tup.sequence,\n",
    "            adjacency_idx_1=tup.adjacency_idx_1,\n",
    "            adjacency_idx_2=tup.adjacency_idx_2,\n",
    "            distances=tup.distances,\n",
    "            target=0\n",
    "        )\n",
    "        row_neg = DataRow(\n",
    "            sequence=tup.sequence_mut,\n",
    "            adjacency_idx_1=tup.adjacency_idx_1,\n",
    "            adjacency_idx_2=tup.adjacency_idx_2,\n",
    "            distances=tup.distances,\n",
    "            target=tup.ddg_exp,\n",
    "        )\n",
    "\n",
    "        # DataSet\n",
    "        random_state = np.random.RandomState()\n",
    "        dataset_pos = dataset_to_gan(row_to_dataset(row_pos, permute=True, random_state=random_state))\n",
    "        dataset_neg = dataset_to_gan(row_to_dataset(row_neg, permute=True, random_state=random_state))\n",
    "\n",
    "        return dataset_pos, dataset_neg, tup.ddg_exp\n",
    "        assert dataset_pos.adjs == dataset_neg.adjs\n",
    "        dataset = (\n",
    "            dataset_pos.seqs + dataset_neg.seqs,\n",
    "            dataset_neg.targets,\n",
    "            dataset_neg.meta,\n",
    "        )\n",
    "        \n",
    "        # DataVar\n",
    "        datavar = self.dataset_to_datavar(dataset)\n",
    "        return datavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProthermData(Dataset):\n",
    "    \n",
    "    def __init__(self, input_file, transform) -> None:\n",
    "        input_df = pq.read_table(input_file).to_pandas()\n",
    "        input_df['sequence'] = input_df['qseq']\n",
    "        # input_df['sequence_mut'] = input_df.apply(mutate_sequence, axis=1)\n",
    "        input_df['sequence_mut'] = input_df['qseq_mutation']\n",
    "        input_df['adjacency_idx_1'] = input_df['residue_idx_1_corrected']\n",
    "        input_df['adjacency_idx_2'] = input_df['residue_idx_2_corrected']\n",
    "        \n",
    "        columns = [\"sequence\", \"sequence_mut\", \"adjacency_idx_1\", \"adjacency_idx_2\", \"distances\", \"ddg_exp\"]\n",
    "        self.tuples = list(input_df[columns].itertuples())\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tuples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tup = self.tuples[index]\n",
    "        datapoint = self.transform(tup)\n",
    "        return datapoint\n",
    "\n",
    "dataset = ProthermData(\n",
    "    input_file,\n",
    "    transform=transforms.Compose([\n",
    "        TupleToDataSet(net_tl.dataset_to_datavar),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pos, dataset_neg, target = dataset[0]\n",
    "dataset_pos.adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_neg.adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_pos.adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try transformations like shuffle by same amount\n",
    "dataloader = DataLoader(dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_df[['sequence', 'sequence_mut', 'adjacency_idx_1', 'adjacency_idx_2', 'distances', 'ddg_exp']].copy()\n",
    "\n",
    "from pagnn.dataset import dataset_to_gan, row_to_dataset\n",
    "\n",
    "for row in df.itertuples():\n",
    "    row_pos = DataRow\n",
    "    dataset = dataset_to_gan(row_to_dataset(row, 0))\n",
    "    datavar = net_tl.dataset_to_datavar(dataset)\n",
    "    outputs = net_tl(datavar.seqs, [datavar.adjs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(row):\n",
    "    sequence = row['sequence']\n",
    "    wt = row['mutation'][0]\n",
    "    pos = int(row['mutation'][1:-1])\n",
    "    mut = row['mutation'][-1]\n",
    "    sequence_mut = sequence[:pos - 1] + mut + sequence[pos:]\n",
    "    assert len(sequence) == len(sequence_mut)\n",
    "    return sequence_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['sequence'] = input_df['qseq']\n",
    "input_df['sequence_mut'] = input_df.apply(mutate_sequence, axis=1)\n",
    "# input_df['sequence_mut'] = input_df['qseq_mutation']\n",
    "input_df['adjacency_idx_1'] = input_df['residue_idx_1_corrected']\n",
    "input_df['adjacency_idx_2'] = input_df['residue_idx_2_corrected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    input_df[f'{network_name}_wt'] = helper.predict_with_network(\n",
    "        input_df[['sequence', 'adjacency_idx_1', 'adjacency_idx_2', 'distances']]\n",
    "            .copy(),\n",
    "        network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "        network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "    )\n",
    "    input_df[f'{network_name}_mut'] = helper.predict_with_network(\n",
    "        input_df[['sequence_mut', 'adjacency_idx_1', 'adjacency_idx_2', 'distances']]\n",
    "            .rename(columns={'sequence_mut': 'sequence'}).copy(),\n",
    "        network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "        network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    input_df[f'{network_name}_change'] = (\n",
    "        input_df[f'{network_name}_mut'] -\n",
    "        input_df[f'{network_name}_wt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(input_df, preserve_index=True)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    OUTPUT_PATH.joinpath(\"validation_protherm_dataset.parquet\"),\n",
    "    version='2.0',\n",
    "    flavor='spark',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "424px",
    "left": "26px",
    "top": "141px",
    "width": "187px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
