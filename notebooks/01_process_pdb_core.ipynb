{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:**\n",
    "\n",
    "These jobs must be submitted from the <code>./notebooks</code> folder.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "**Cedar:**\n",
    "\n",
    "```bash\n",
    "NOTEBOOK_PATH=$(realpath 01_process_pdb_core.ipynb) sbatch --array=1-300 --time=24:00:00 --ntasks=1 --cpus-per-task=32 --constraint=skylake --job-name=process-pdb-core --account=rrg-pmkim --output=/scratch/strokach/tmp/log/run-notebook-cpu-%N-%j.log ../scripts/run_notebook_cpu.sh\n",
    "```\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import concurrent.futures.process\n",
    "import gzip\n",
    "import importlib\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import shlex\n",
    "import shutil\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mdtraj\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import tenacity\n",
    "from kmbio import PDB\n",
    "from kmtools import structure_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path(os.getenv(\"CI_JOB_NAME\", \"01_process_pdb_core\"))\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd().expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_DATA_PATH = (\n",
    "    Path(os.environ[\"DATAPKG_OUTPUT_DIR\"]).joinpath(\"pdb-ffindex\", \"2020-01-16\", \"arrow\").resolve(strict=True)\n",
    ")\n",
    "\n",
    "PDB_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scinet\" in socket.gethostname():\n",
    "    CPU_COUNT = 40\n",
    "else:\n",
    "    CPU_COUNT = psutil.cpu_count(logical=False)\n",
    "    \n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJACENCY_NET_DATA_PATH = (\n",
    "    Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"adjacency-net-v2\", \"v0.3\").resolve(strict=True)\n",
    ")\n",
    "\n",
    "ADJACENCY_NET_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = TASK_ID is None\n",
    "\n",
    "if DEBUG:\n",
    "    TASK_ID = 4\n",
    "    TASK_COUNT = 300\n",
    "else:\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "\n",
    "TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PDB_DATA_PATH.joinpath(\"pdb-list.pickle\").open(\"rb\") as fin:\n",
    "    pdb_list = pickle.load(fin)\n",
    "    \n",
    "pdb_data_reader = pa.RecordBatchFileReader(PDB_DATA_PATH.joinpath(\"pdb-mmcif.arrow\"))\n",
    "\n",
    "assert len(pdb_list) == pdb_data_reader.num_record_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = int(np.ceil(len(pdb_list) / TASK_COUNT))\n",
    "task_idx = TASK_ID - 1\n",
    "pdb_chunk = pdb_list[task_idx * chunk_size : (task_idx + 1) * chunk_size]\n",
    "pdb_chunk_idxs = list(range(task_idx * chunk_size, (task_idx + 1) * chunk_size))\n",
    "assert all(pdb_chunk[i] == pdb_list[j] for i, j in enumerate(pdb_chunk_idxs))\n",
    "\n",
    "chunk_size, task_idx, len(pdb_chunk), pdb_chunk[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    pdb_chunk = pdb_chunk[:10]\n",
    "    pdb_chunk_idxs = pdb_chunk_idxs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_from_chain(structure_ref, model_ref, chain):\n",
    "    model = PDB.Model(model_ref.id, model_ref.serial_num)\n",
    "    model.add(chain)\n",
    "    structure = PDB.Structure(structure_ref.id)\n",
    "    structure.add(model)\n",
    "    assert len(list(structure.chains)) == 1\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdb_core(pdb_id, pdb_idx):\n",
    "    # Load data\n",
    "    pdb_data_reader = pa.RecordBatchFileReader(PDB_DATA_PATH.joinpath(\"pdb-mmcif.arrow\"))\n",
    "    pdb_data = pdb_data_reader.get_record_batch(pdb_idx).to_pydict()\n",
    "    assert pdb_data[\"pdb_id\"][0] == pdb_id\n",
    "\n",
    "    # Create structure from data\n",
    "    buf = io.StringIO()\n",
    "    buf.write(gzip.decompress(pdb_data[\"mmcif_data\"][0]).decode())\n",
    "    use_auth_id = False\n",
    "    try:\n",
    "        buf.seek(0)\n",
    "        bioassembly_id = True\n",
    "        structure = PDB.MMCIFParser(use_auth_id=use_auth_id).get_structure(buf, bioassembly_id=bioassembly_id)\n",
    "    except PDB.BioassemblyError as e:\n",
    "        print(f\"Encountered error when parsing pdb {pdb_idx} ('{pdb_id}'): {e!s}.\")\n",
    "        buf.seek(0)\n",
    "        bioassembly_id = False\n",
    "        structure = PDB.MMCIFParser(use_auth_id=use_auth_id).get_structure(buf, bioassembly_id=bioassembly_id)\n",
    "\n",
    "    results = []\n",
    "    _seen = set()\n",
    "    for model_idx, model in enumerate(structure):\n",
    "        for chain_idx, chain in enumerate(model):\n",
    "            aa_sequence = structure_tools.get_chain_sequence(chain)\n",
    "            if aa_sequence in _seen:\n",
    "                continue\n",
    "            _seen.add(aa_sequence)\n",
    "            if len(aa_sequence.strip()) < 5:\n",
    "                continue\n",
    "\n",
    "            schain = structure_from_chain(structure, model, chain.copy())\n",
    "\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdb\") as pdb_file:\n",
    "                PDB.save(schain, pdb_file.name)\n",
    "                traj = mdtraj.load(pdb_file.name)\n",
    "            assert aa_sequence == traj.top.to_fasta()[0]\n",
    "\n",
    "            residue_df = helper.construct_residue_df(traj)\n",
    "            helper.validate_residue_df(residue_df)\n",
    "\n",
    "            residue_pairs_df = helper.construct_residue_pairs_df(traj)\n",
    "            helper.validate_residue_pairs_df(residue_pairs_df)\n",
    "\n",
    "            result = {\n",
    "                \"pdb_id\": [pdb_id],\n",
    "                \"pdb_idx\": [pdb_idx],\n",
    "                \"use_auth_id\": [use_auth_id],\n",
    "                \"bioassembly_id\": [bioassembly_id],\n",
    "                \"model_idx\": [model_idx],\n",
    "                \"model_id\": [model.id],\n",
    "                \"chain_idx\": [chain_idx],\n",
    "                \"chain_id\": [chain.id],\n",
    "                **helper.residue_df_to_row(residue_df),\n",
    "                **helper.residue_pairs_df_to_row(residue_pairs_df),\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(pdb_id, pdb_idx):\n",
    "    try:\n",
    "        return process_pdb_core(pdb_id, pdb_idx), []\n",
    "    except Exception as error:\n",
    "        return [], [{\"pdb_id\": [pdb_id], \"pdb_idx\": [pdb_idx], \"error_type\": [str(type(error))], \"error\": [str(error)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = worker(pdb_chunk[0], pdb_chunk_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with concurrent.futures.ProcessPoolExecutor() as pool:\n",
    "#     futures = pool.map(worker, pdb_chunk[:10], pdb_chunk_idxs[:10])\n",
    "#     for result in tqdm(futures, total=100):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = ADJACENCY_NET_DATA_PATH.joinpath(\"pdb-core\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir.joinpath(f\"pdb-core-{TASK_ID}-{TASK_COUNT}.arrow\")\n",
    "\n",
    "output_dir_failed = ADJACENCY_NET_DATA_PATH.joinpath(\"pdb-core-failed\")\n",
    "output_dir_failed.mkdir(exist_ok=True)\n",
    "output_file_failed = output_dir_failed.joinpath(f\"pdb-core-{TASK_ID}-{TASK_COUNT}-failed.arrow\")\n",
    "\n",
    "output_file, output_file_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = None\n",
    "writer_failed = None\n",
    "with concurrent.futures.ProcessPoolExecutor(CPU_COUNT) as pool:\n",
    "    futures = pool.map(worker, pdb_chunk, pdb_chunk_idxs)\n",
    "    for (results, results_failed) in tqdm(futures, total=len(pdb_chunk)):\n",
    "        for result in results:\n",
    "            if writer is None:\n",
    "                batch = pa.RecordBatch.from_arrays(list(result.values()), list(result.keys()))\n",
    "                writer = pa.RecordBatchFileWriter(output_file, batch.schema)\n",
    "            batch = pa.RecordBatch.from_arrays(list(result.values()), list(result.keys()))\n",
    "            writer.write_batch(batch)\n",
    "        for result_failed in results_failed:\n",
    "            if writer_failed is None:\n",
    "                batch = pa.RecordBatch.from_arrays(list(result_failed.values()), list(result_failed.keys()))\n",
    "                writer_failed = pa.RecordBatchFileWriter(output_file_failed, batch.schema)\n",
    "            batch = pa.RecordBatch.from_arrays(list(result_failed.values()), list(result_failed.keys()))\n",
    "            writer_failed.write_batch(batch)\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "if writer_failed is not None:\n",
    "    writer_failed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_file.is_file():\n",
    "    reader = pa.RecordBatchFileReader(output_file)\n",
    "    print(f\"Number of successful chains: {reader.num_record_batches}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_file_failed.is_file():\n",
    "    reader_failed = pa.RecordBatchFileReader(output_file_failed)\n",
    "    print(f\"Number of failed PDBs: {reader_failed.num_record_batches}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `_SUCCESS` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with output_file.parent.joinpath(\"_SUCCESS\").open(\"wt\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482px",
    "left": "34.9826px",
    "top": "126.979px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
