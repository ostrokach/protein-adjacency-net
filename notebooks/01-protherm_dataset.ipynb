{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Generate adjancency matrices for the Protherm training set directly from PDBs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import importlib\n",
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "import sys\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import kmbio.PDB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from kmtools import structure_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = 'protherm_dataset'\n",
    "NOTEBOOK_PATH = Path(NOTEBOOK_NAME)\n",
    "\n",
    "NOTEBOOK_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG = \"CI\" not in os.environ\n",
    "DEBUG = False\n",
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "DEBUG, TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABIN_PATH = Path(os.environ['DATABIN_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROSETTA_RESULTS = {}\n",
    "\n",
    "with pd.HDFStore(DATABIN_PATH.joinpath('elapsam_feature_engineering/v0.1.0/rosetta.h5').as_posix(), 'r') as store:\n",
    "    for key in store:\n",
    "        ROSETTA_RESULTS[key.strip('/')] = store[key][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROSETTA_RESULTS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROSETTA_RESULTS['cartesian_ddg-talaris2014_cart-1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosetta_results_df = None\n",
    "\n",
    "for key, df in ROSETTA_RESULTS.items():\n",
    "    df = df.rename(columns={'ddg': key})\n",
    "    if rosetta_results_df is None:\n",
    "        rosetta_results_df = df\n",
    "    else:\n",
    "        assert (rosetta_results_df['ddg_exp'].values == df['ddg_exp'].values).all()\n",
    "        rosetta_results_df = rosetta_results_df.merge(\n",
    "            df.drop('ddg_exp', axis=1), on=['filename-wt', 'pdb_chain', 'mutation'], how='outer')\n",
    "\n",
    "rosetta_results_df = rosetta_results_df.rename(columns=lambda c: c.replace('-', '_').strip('_'))\n",
    "display(rosetta_results_df.head())\n",
    "print(rosetta_results_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRUCTURE_PATH = NOTEBOOK_PATH.joinpath('structures')\n",
    "STRUCTURE_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(STRUCTURE_PATH)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_local_filename(filename):\n",
    "    return STRUCTURE_PATH.joinpath(op.basename(filename)).absolute().as_posix()\n",
    "\n",
    "get_local_filename(rosetta_results_df['filename_wt'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = rosetta_results_df['filename_wt'].drop_duplicates().tolist()\n",
    "\n",
    "local_filename_wt = []\n",
    "for i, filename in enumerate(file_list):\n",
    "    if i % 200 == 0:\n",
    "        print(i)\n",
    "    new_filename = STRUCTURE_PATH.joinpath(op.basename(filename))\n",
    "    filename = filename.replace(\n",
    "        \"/home/kimlab2/database_data/biological-data-warehouse\",\n",
    "        Path(\"~/datapkg\").expanduser().as_posix(),\n",
    "    )\n",
    "    local_filename = get_local_filename(filename)\n",
    "    if not op.isfile(local_filename):\n",
    "        shutil.copy(filename, local_filename)\n",
    "    local_filename_wt.append(local_filename)   \n",
    "\n",
    "rosetta_results_df['local_filename_wt'] = local_filename_wt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rosetta_results_df = rosetta_results_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract adjacencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seq_and_adj(row):\n",
    "    domain, result_df = helper.get_interaction_dataset_wdistances(\n",
    "        row.local_filename_wt, 0, row.pdb_chain, r_cutoff=12)\n",
    "    domain_sequence = structure_tools.get_chain_sequence(domain)\n",
    "    assert max(result_df['residue_idx_1'].values) < len(domain_sequence)\n",
    "    assert max(result_df['residue_idx_2'].values) < len(domain_sequence)\n",
    "    result = {\n",
    "        'sequence': domain_sequence,\n",
    "        'residue_idx_1': result_df['residue_idx_1'].values,\n",
    "        'residue_idx_2': result_df['residue_idx_2'].values,\n",
    "        'distances': result_df['distance'].values,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(row_dict):\n",
    "    row = helper.to_namedtuple(row_dict)\n",
    "    result = extract_seq_and_adj(row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"kmbio.PDB.core.atom\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"local_filename_wt\", \"pdb_chain\"]\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(psutil.cpu_count(logical=False)) as pool:\n",
    "    futures = pool.map(worker, (t._asdict() for t in rosetta_results_df[columns].itertuples()))\n",
    "    results = list(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protherm_validaton_dataset = rosetta_results_df.copy()\n",
    "protherm_validaton_dataset = protherm_validaton_dataset.rename(columns={'pdb_chain': 'chain_id'})\n",
    "                                                               \n",
    "protherm_validaton_dataset['structure_id'] = [\n",
    "    Path(filename).name[3:7] for filename in protherm_validaton_dataset[\"filename_wt\"]\n",
    "]\n",
    "protherm_validaton_dataset['model_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protherm_validaton_dataset['qseq'] = [result[\"sequence\"] for result in results]\n",
    "protherm_validaton_dataset['residue_idx_1_corrected'] = [result[\"residue_idx_1\"] for result in results]\n",
    "protherm_validaton_dataset['residue_idx_2_corrected'] = [result[\"residue_idx_2\"] for result in results]\n",
    "protherm_validaton_dataset['distances'] = [result[\"distances\"] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation_matches_sequence(mutation, sequence):\n",
    "    return sequence[int(mutation[1:-1]) - 1] == mutation[0]\n",
    "\n",
    "\n",
    "protherm_validaton_dataset['mutation_matches_sequence'] = [\n",
    "    mutation_matches_sequence(mutation, sequence)\n",
    "    for mutation, sequence\n",
    "    in protherm_validaton_dataset[['mutation', 'qseq']].values\n",
    "]\n",
    "assert protherm_validaton_dataset['mutation_matches_sequence'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mutation(sequence, mutation):\n",
    "    wt, pos, mut = mutation[0], int(mutation[1:-1]), mutation[-1]\n",
    "    assert sequence[pos - 1] == wt\n",
    "    sequence_mut = sequence[:pos - 1] + mut + sequence[pos:]\n",
    "    assert sequence_mut[pos - 1] == mut\n",
    "    assert len(sequence) == len(sequence_mut)\n",
    "    return sequence_mut\n",
    "\n",
    "protherm_validaton_dataset['qseq_mutation'] = [\n",
    "    apply_mutation(sequence, mutation)\n",
    "    for mutation, sequence\n",
    "    in protherm_validaton_dataset[['mutation', 'qseq']].values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not protherm_validaton_dataset.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'structure_id', 'model_id', 'chain_id', 'qseq', 'qseq_mutation', 'ddg_exp', \n",
    "    'residue_idx_1_corrected', 'residue_idx_2_corrected', 'distances',\n",
    "]\n",
    "\n",
    "for column in columns:\n",
    "    assert column in protherm_validaton_dataset.columns, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(\n",
    "    pa.Table.from_pandas(protherm_validaton_dataset, preserve_index=False),\n",
    "    OUTPUT_PATH.joinpath('protherm_validaton_dataset.parquet').as_posix(),\n",
    "    version='2.0', flavor='spark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_wt_counter = Counter(protherm_validaton_dataset['mutation'].str[0])\n",
    "aa_mut_counter = Counter(protherm_validaton_dataset['mutation'].str[-1])\n",
    "\n",
    "labels = list(aa_wt_counter)\n",
    "aa_wt = [aa_wt_counter[l] for l in labels]\n",
    "aa_mut = [aa_mut_counter[l] for l in labels]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 0.3\n",
    "\n",
    "with plt.rc_context(rc={'figure.figsize': (8, 5), 'font.size': 14}):\n",
    "    plt.bar(indexes - 0.15 , aa_wt, width, label=\"wt\")\n",
    "    plt.bar(indexes + 0.15, aa_mut, width, label=\"mut\")\n",
    "    plt.xticks(indexes, labels)\n",
    "    plt.ylabel(\"Number of occurrences\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "401px",
    "left": "73.6094px",
    "right": "20px",
    "top": "143.344px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
