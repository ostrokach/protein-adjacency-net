{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We need to rerun `add_adjacency_distances.ipynb` code for the following domains:\n",
    "\n",
    "***training_dataset***\n",
    "\n",
    "~72% of rows finished.\n",
    "\n",
    "```raw\n",
    "[('database_id=G3DSA%3A3.40.50.1440', 62807),\n",
    " ('database_id=G3DSA%3A3.20.80.10', 43126),\n",
    " ('database_id=G3DSA%3A3.30.1050.10', 34935),\n",
    " ('database_id=G3DSA%3A3.30.110.60', 11123),\n",
    " ('database_id=G3DSA%3A1.10.10.440', 12939),\n",
    " ('database_id=G3DSA%3A1.10.10.41', 2895),\n",
    " ('database_id=G3DSA%3A3.30.1140.32', 17973),\n",
    " ('database_id=G3DSA%3A3.30.1120.40', 3065),\n",
    " ('database_id=G3DSA%3A2.60.40.1200', 53),\n",
    " ('database_id=G3DSA%3A1.10.10.250', 18310),\n",
    " ('database_id=G3DSA%3A2.60.40.1090', 45878),\n",
    " ('database_id=G3DSA%3A3.50.7.10', 57672),\n",
    " ('database_id=G3DSA%3A3.50.70.10', 6224),\n",
    " ('database_id=G3DSA%3A1.10.10.10', 3400746),\n",
    " ('database_id=G3DSA%3A3.20.20.70', 1701341),\n",
    " ('database_id=G3DSA%3A1.25.40.10', 2105462),\n",
    " ('database_id=G3DSA%3A3.20.90.10', 5021),\n",
    " ('database_id=G3DSA%3A3.40.640.10', 940256),\n",
    " ('database_id=G3DSA%3A1.10.10.190', 11),\n",
    " ('database_id=G3DSA%3A1.10.10.410', 28270),\n",
    " ('database_id=G3DSA%3A1.10.10.180', 1311),\n",
    " ('database_id=G3DSA%3A3.30.1040.10', 11),\n",
    " ('database_id=G3DSA%3A2.60.40.1240', 16791),\n",
    " ('database_id=G3DSA%3A1.10.10.500', 1171),\n",
    " ('database_id=G3DSA%3A3.30.110.40', 21055),\n",
    " ('database_id=G3DSA%3A3.50.50.60', 1600839),\n",
    " ('database_id=G3DSA%3A1.10.10.390', 428),\n",
    " ('database_id=G3DSA%3A2.60.40.10', 2604549),\n",
    " ('database_id=G3DSA%3A3.30.1120.30', 4708),\n",
    " ('database_id=G3DSA%3A1.10.10.400', 18468),\n",
    " ('database_id=G3DSA%3A2.60.40.1120', 184571),\n",
    " ('database_id=G3DSA%3A1.10.10.430', 1060)]\n",
    "```\n",
    "\n",
    "***validation_dataset***\n",
    "\n",
    "~82% of rows finished.\n",
    "\n",
    "```raw\n",
    "[('database_id=G3DSA%3A2.102.10.10', 2531),\n",
    " ('database_id=G3DSA%3A2.20.140.10', 182),\n",
    " ('database_id=G3DSA%3A1.25.40.270', 340),\n",
    " ('database_id=G3DSA%3A2.10.25.30', 23),\n",
    " ('database_id=G3DSA%3A2.30.29.30', 30432),\n",
    " ('database_id=G3DSA%3A1.25.40.20', 40925),\n",
    " ('database_id=G3DSA%3A2.120.10.90', 1824),\n",
    " ('database_id=G3DSA%3A2.30.170.40', 1510),\n",
    " ('database_id=G3DSA%3A2.100.10.20', 45),\n",
    " ('database_id=G3DSA%3A2.20.90.10', 4),\n",
    " ('database_id=G3DSA%3A2.30.130.10', 2377),\n",
    " ('database_id=G3DSA%3A1.20.59.10', 964),\n",
    " ('database_id=G3DSA%3A2.170.14.10', 59),\n",
    " ('database_id=G3DSA%3A2.20.170.10', 10),\n",
    " ('database_id=G3DSA%3A2.100.10.30', 349),\n",
    " ('database_id=G3DSA%3A2.10.290.10', 180),\n",
    " ('database_id=G3DSA%3A1.25.10.30', 970),\n",
    " ('database_id=G3DSA%3A2.30.18.10', 189)]\n",
    "```\n",
    "\n",
    "***test_dataset***\n",
    "\n",
    "~95.6% of rows finished.\n",
    "\n",
    "```raw\n",
    "[('database_id=G3DSA%3A2.40.290.10', 610),\n",
    " ('database_id=G3DSA%3A2.30.30.390', 56),\n",
    " ('database_id=G3DSA%3A2.40.200.10', 14),\n",
    " ('database_id=G3DSA%3A2.40.230.10', 589),\n",
    " ('database_id=G3DSA%3A2.40.230.20', 399),\n",
    " ('database_id=G3DSA%3A2.40.270.10', 8268),\n",
    " ('database_id=G3DSA%3A2.40.180.10', 5573)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import concurrent.futures.process\n",
    "import importlib\n",
    "import os\n",
    "import shlex\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import yaml\n",
    "\n",
    "from kmbio import PDB\n",
    "from kmtools import structure_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run spark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path(os.getenv(\"CI_JOB_NAME\", \"add_adjacency_distances_retry\"))\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd().expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "\n",
    "if DEBUG:\n",
    "    ADJACENCY_MATRIX_PARQUET_PATH = (\n",
    "        Path(os.getenv(\"DATAPKG_OUTPUT_DIR\"))\n",
    "        .joinpath(\"adjacency-net-v2\", \"master\", \"training_dataset\", \"adjacency_matrix.parquet\")\n",
    "    )\n",
    "else:\n",
    "    ADJACENCY_MATRIX_PARQUET_PATH = (\n",
    "        Path(os.getenv(\"ADJACENCY_MATRIX_PARQUET_PATH\")).expanduser()\n",
    "    )\n",
    "\n",
    "assert ADJACENCY_MATRIX_PARQUET_PATH.is_dir()\n",
    "\n",
    "DEBUG, ADJACENCY_MATRIX_PARQUET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAILED_DOMAIN_NAMES = {\n",
    "    'training_dataset': [t[0] for t in \n",
    "        [('database_id=G3DSA%3A3.40.50.1440', 62807),\n",
    "         ('database_id=G3DSA%3A3.20.80.10', 43126),\n",
    "         ('database_id=G3DSA%3A3.30.1050.10', 34935),\n",
    "         ('database_id=G3DSA%3A3.30.110.60', 11123),\n",
    "         ('database_id=G3DSA%3A1.10.10.440', 12939),\n",
    "         ('database_id=G3DSA%3A1.10.10.41', 2895),\n",
    "         ('database_id=G3DSA%3A3.30.1140.32', 17973),\n",
    "         ('database_id=G3DSA%3A3.30.1120.40', 3065),\n",
    "         ('database_id=G3DSA%3A2.60.40.1200', 53),\n",
    "         ('database_id=G3DSA%3A1.10.10.250', 18310),\n",
    "         ('database_id=G3DSA%3A2.60.40.1090', 45878),\n",
    "         ('database_id=G3DSA%3A3.50.7.10', 57672),\n",
    "         ('database_id=G3DSA%3A3.50.70.10', 6224),\n",
    "         ('database_id=G3DSA%3A1.10.10.10', 3400746),\n",
    "         ('database_id=G3DSA%3A3.20.20.70', 1701341),\n",
    "         ('database_id=G3DSA%3A1.25.40.10', 2105462),\n",
    "         ('database_id=G3DSA%3A3.20.90.10', 5021),\n",
    "         ('database_id=G3DSA%3A3.40.640.10', 940256),\n",
    "         ('database_id=G3DSA%3A1.10.10.190', 11),\n",
    "         ('database_id=G3DSA%3A1.10.10.410', 28270),\n",
    "         ('database_id=G3DSA%3A1.10.10.180', 1311),\n",
    "         ('database_id=G3DSA%3A3.30.1040.10', 11),\n",
    "         ('database_id=G3DSA%3A2.60.40.1240', 16791),\n",
    "         ('database_id=G3DSA%3A1.10.10.500', 1171),\n",
    "         ('database_id=G3DSA%3A3.30.110.40', 21055),\n",
    "         ('database_id=G3DSA%3A3.50.50.60', 1600839),\n",
    "         ('database_id=G3DSA%3A1.10.10.390', 428),\n",
    "         ('database_id=G3DSA%3A2.60.40.10', 2604549),\n",
    "         ('database_id=G3DSA%3A3.30.1120.30', 4708),\n",
    "         ('database_id=G3DSA%3A1.10.10.400', 18468),\n",
    "         ('database_id=G3DSA%3A2.60.40.1120', 184571),\n",
    "         ('database_id=G3DSA%3A1.10.10.430', 1060)]\n",
    "    ],\n",
    "    'validation_dataset': [t[0] for t in \n",
    "        [('database_id=G3DSA%3A2.102.10.10', 2531),\n",
    "         ('database_id=G3DSA%3A2.20.140.10', 182),\n",
    "         ('database_id=G3DSA%3A1.25.40.270', 340),\n",
    "         ('database_id=G3DSA%3A2.10.25.30', 23),\n",
    "         ('database_id=G3DSA%3A2.30.29.30', 30432),\n",
    "         ('database_id=G3DSA%3A1.25.40.20', 40925),\n",
    "         ('database_id=G3DSA%3A2.120.10.90', 1824),\n",
    "         ('database_id=G3DSA%3A2.30.170.40', 1510),\n",
    "         ('database_id=G3DSA%3A2.100.10.20', 45),\n",
    "         ('database_id=G3DSA%3A2.20.90.10', 4),\n",
    "         ('database_id=G3DSA%3A2.30.130.10', 2377),\n",
    "         ('database_id=G3DSA%3A1.20.59.10', 964),\n",
    "         ('database_id=G3DSA%3A2.170.14.10', 59),\n",
    "         ('database_id=G3DSA%3A2.20.170.10', 10),\n",
    "         ('database_id=G3DSA%3A2.100.10.30', 349),\n",
    "         ('database_id=G3DSA%3A2.10.290.10', 180),\n",
    "         ('database_id=G3DSA%3A1.25.10.30', 970),\n",
    "         ('database_id=G3DSA%3A2.30.18.10', 189)]\n",
    "    ],\n",
    "    'test_dataset': [t[0] for t in     \n",
    "        [('database_id=G3DSA%3A2.40.290.10', 610),\n",
    "         ('database_id=G3DSA%3A2.30.30.390', 56),\n",
    "         ('database_id=G3DSA%3A2.40.200.10', 14),\n",
    "         ('database_id=G3DSA%3A2.40.230.10', 589),\n",
    "         ('database_id=G3DSA%3A2.40.230.20', 399),\n",
    "         ('database_id=G3DSA%3A2.40.270.10', 8268),\n",
    "         ('database_id=G3DSA%3A2.40.180.10', 5573)]\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['pdb-ffindex'] = {\n",
    "    'pdb_mmcif_ffindex': (\n",
    "        Path(os.environ['DATAPKG_OUTPUT_DIR'])\n",
    "        .joinpath(\"pdb-ffindex\", \"master\", \"pdb_mmcif_ffindex\", \"pdb-mmcif\")\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(os.getenv(\"DATAPKG_OUTPUT_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(sum(\n",
    "    (list(\n",
    "        Path(os.getenv(\"DATAPKG_OUTPUT_DIR\"))\n",
    "        .joinpath(\"adjacency-net-v2\", \"master\", dataset_name, \"adjacency_matrix.parquet\", domain_name)\n",
    "        .glob(\"*.parquet\")\n",
    "    )\n",
    "    for dataset_name in [\"training_dataset\", \"validation_dataset\", \"test_dataset\"]\n",
    "    for domain_name in FAILED_DOMAIN_NAMES[dataset_name])\n",
    ", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{f.parent.parent for f in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pq.ParquetFile(files[0])\n",
    "    .read_row_group(0, use_pandas_metadata=True)\n",
    "    .to_pandas(integer_object_nulls=True)\n",
    "    .set_index(\"__index_level_0__\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(data):\n",
    "    row = helper.to_namedtuple(data)\n",
    "    results = {}\n",
    "    try:\n",
    "        results['residue_idx_1'], results['residue_idx_2'], results['distances'] = (\n",
    "            helper.get_adjacency_with_distances(\n",
    "                row, max_cutoff=12, min_cutoff=None, structure_url_prefix=STRUCTURE_URL_PREFIX\n",
    "            )\n",
    "        )\n",
    "#         results['residue_idx_1'] = results['residue_idx_1'].tolist()\n",
    "#         results['residue_idx_2'] = results['residue_idx_2'].tolist()\n",
    "#         results['distances'] = results['distances'].tolist()\n",
    "        results[\"error\"] = None\n",
    "    except Exception as e:\n",
    "        results = {\n",
    "            \"residue_idx_1\": None,\n",
    "            \"residue_idx_2\": None,\n",
    "            \"distances\": None,\n",
    "            \"error\": f\"{type(e)}: {e}\",\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run worker for single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "row = next(islice(df.itertuples(), start, start + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRUCTURE_URL_PREFIX = f\"ff://{DATAPKG['pdb-ffindex']['pdb_mmcif_ffindex']}?\"\n",
    "STRUCTURE_URL_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# residue_idx_1, residue_idx_2, distance = helper.get_adjacency_with_distances(\n",
    "#     row, max_cutoff=12, min_cutoff=None, structure_url_prefix=STRUCTURE_URL_PREFIX\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker(row._asdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file(file, suffix):\n",
    "    file_parts = list(file.parts)\n",
    "    file_parts[-4] = file_parts[-4] + suffix\n",
    "    new_file = Path(*file_parts)\n",
    "    return new_file\n",
    "\n",
    "get_new_file(files[0], \"_wdistances_failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine subset of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    files_arrow_not_implemented = set()\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            ds = pq.ParquetFile(file)\n",
    "            table = ds.read_row_group(0, use_pandas_metadata=True)\n",
    "        except pa.ArrowNotImplementedError:\n",
    "            files_arrow_not_implemented.add(file)\n",
    "\n",
    "    files = [f for f in files if f not in files_arrow_not_implemented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    files_to_run = files[:10]\n",
    "else:\n",
    "    files_to_run = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files_to_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_to_run:\n",
    "    ds = pq.ParquetFile(file)\n",
    "    new_file = get_new_file(file, \"_wdistances\")\n",
    "    new_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    writer_failed_is_initialized = False\n",
    "    using_spark_sql = False\n",
    "    for row_group in range(ds.num_row_groups):\n",
    "        # Read row group (or entire file using Spark SQL if PyArrow fails)\n",
    "        try:\n",
    "            df = (\n",
    "                ds.read_row_group(row_group, use_pandas_metadata=True)\n",
    "                .to_pandas(integer_object_nulls=True)\n",
    "                .set_index(\"__index_level_0__\")\n",
    "            )\n",
    "        except pa.ArrowNotImplementedError:\n",
    "            using_spark_sql = True\n",
    "            df = spark.read.parquet(file.as_posix()).toPandas()\n",
    "\n",
    "        try:\n",
    "            with concurrent.futures.ProcessPoolExecutor(psutil.cpu_count(logical=False)) as pool:\n",
    "                futures = pool.map(\n",
    "                    worker,\n",
    "                    (t._asdict() for t in df[helper.GET_ADJACENCY_WITH_DISTANCES_ROW_ATTRIBUTES].itertuples()),\n",
    "                    chunksize=1)\n",
    "                results = list(futures)\n",
    "                results_df = pd.DataFrame(results)\n",
    "                df[\"residue_idx_1_corrected\"] = results_df[\"residue_idx_1\"].values\n",
    "                df[\"residue_idx_2_corrected\"] = results_df[\"residue_idx_2\"].values\n",
    "                df[\"distances\"] = results_df[\"distances\"].values\n",
    "                df[\"error_adding_distances\"] = results_df[\"error\"].values\n",
    "                num_errors = df[\"error_adding_distances\"].notnull().sum()\n",
    "                if num_errors:\n",
    "                    print(f\"Encountered {num_errors} errors when parsing file '{file}'.\")\n",
    "        except concurrent.futures.process.BrokenProcessPool as e:\n",
    "            warnings.warn(\n",
    "                f\"ProcessPool crashed while processing row_group '{row_group}' in file '{file}'.\"\n",
    "                f\"The error is '{type(e)}': {e}.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        df_succeeded = df[\n",
    "            df[[\"residue_idx_1_corrected\", \"residue_idx_2_corrected\", \"distances\"]].notnull().all(axis=1)\n",
    "        ]\n",
    "        df_failed =  df[\n",
    "            df[[\"residue_idx_1_corrected\", \"residue_idx_2_corrected\", \"distances\"]].isnull().all(axis=1)\n",
    "        ]\n",
    "        assert len(df_succeeded) + len(df_failed) == len(df)\n",
    "\n",
    "        # Write successful results\n",
    "        table = pa.Table.from_pandas(df_succeeded, preserve_index=True)\n",
    "        if row_group == 0:\n",
    "            writer = pq.ParquetWriter(new_file, table.schema, version=\"2.0\", flavor=\"spark\")\n",
    "        writer.write_table(table)\n",
    "\n",
    "        # Write failed results\n",
    "        if not df_failed.empty:\n",
    "            table_failed = pa.Table.from_pandas(df_failed, preserve_index=True)\n",
    "            if not writer_failed_is_initialized:\n",
    "                new_file_failed = get_new_file(file, \"_wdistances_failed\")\n",
    "                new_file_failed.parent.mkdir(parents=True, exist_ok=True)\n",
    "                writer_failed = pq.ParquetWriter(new_file_failed, table_failed.schema, version=\"2.0\", flavor=\"spark\")\n",
    "                writer_failed_is_initialized = True\n",
    "            writer_failed.write_table(table_failed)\n",
    "\n",
    "        # print(len(df_succeeded), len(df_failed))\n",
    "\n",
    "        if using_spark_sql:\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    if writer_failed_is_initialized:\n",
    "        writer_failed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    display(df.head())\n",
    "\n",
    "    # Make sure that the file we wrote makes sense\n",
    "    for file in files_to_run:\n",
    "        new_file = get_new_file(file)\n",
    "\n",
    "        ds = pq.ParquetFile(file)\n",
    "        ds_new = pq.ParquetFile(new_file)\n",
    "        assert ds.num_row_groups == ds_new.num_row_groups\n",
    "\n",
    "        for row_group in range(ds.num_row_groups):\n",
    "            df = (\n",
    "                ds.read_row_group(0, use_pandas_metadata=True)\n",
    "                .to_pandas(integer_object_nulls=True)\n",
    "                .set_index(\"__index_level_0__\")\n",
    "            )\n",
    "            df_new = (\n",
    "                ds_new.read_row_group(0, use_pandas_metadata=True)\n",
    "                .to_pandas(integer_object_nulls=True)\n",
    "#                 .set_index(\"__index_level_0__\")\n",
    "            )\n",
    "            shared_columns = [\n",
    "                c for c in df.columns\n",
    "                if c in df_new.columns\n",
    "                and c not in [\n",
    "                    'a2b', 'b2a', 'residue_idx_1', 'residue_idx_2',\n",
    "                    'residue_id_1', 'residue_id_2', 'residue_aa_1', 'residue_aa_2',\n",
    "                    \"residue_idx_1_corrected\", \"residue_idx_2_corrected\"]\n",
    "            ]\n",
    "            assert (df[shared_columns] == df_new[shared_columns]).all().all()\n",
    "            assert all(\n",
    "                (l1 == l2).all() \n",
    "                for l1, l2\n",
    "                in zip(df[\"residue_id_1\"].values, df_new[\"residue_id_1\"].values)\n",
    "            )\n",
    "            assert all(\n",
    "                (l1.shape != l2.shape or not (l1 == l2).all())\n",
    "                for l1, l2\n",
    "                in zip(df[\"residue_idx_1_corrected\"].values, df_new[\"residue_idx_1_corrected\"].values)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482px",
    "left": "34.9826px",
    "top": "126.979px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
