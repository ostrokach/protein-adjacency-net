{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path('validation_homology_models_combined')\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_VERSION = os.getenv(\"PROJECT_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    PROJECT_VERSION = \"0.1\"\n",
    "else:\n",
    "    assert PROJECT_VERSION is not None\n",
    "    \n",
    "PROJECT_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     %load_ext autoreload\n",
    "#     %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['validation_homology_models'] = sorted(\n",
    "    Path(os.environ['DATAPKG_OUTPUT_DIR'])\n",
    "    .joinpath(\"adjacency-net-v2\", f\"v{PROJECT_VERSION}\", \"validation_homology_models\")\n",
    "    .glob(\"*/*_dataset.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG['validation_homology_models']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `homology_models_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = None\n",
    "\n",
    "\n",
    "def assert_eq(a1, a2):\n",
    "    if isinstance(a1[0], np.ndarray):\n",
    "        for b1, b2 in zip(a1, a2):\n",
    "            b1 = b1[~np.isnan(b1)]\n",
    "            b2 = b2[~np.isnan(b2)]\n",
    "            assert len(b1) == len(b2)\n",
    "            assert (b1 == b2).all()\n",
    "    else:\n",
    "        assert (a1 == a2).all()\n",
    "            \n",
    "\n",
    "for file in DATAPKG['validation_homology_models']:\n",
    "    df = pq.read_table(file, use_pandas_metadata=True).to_pandas(integer_object_nulls=True)\n",
    "    df.drop(pd.Index(['error']), axis=1, inplace=True)\n",
    "    if validation_df is None:\n",
    "        validation_df = df\n",
    "    else:\n",
    "        validation_df = (\n",
    "            validation_df\n",
    "            .merge(df, how=\"outer\", left_index=True, right_index=True, validate=\"1:1\", suffixes=(\"\", \"_dup\"))\n",
    "        )\n",
    "        for col in validation_df.columns:\n",
    "            if col.endswith(f\"_dup\"):\n",
    "                col_ref = col[:-4]\n",
    "                assert_eq(validation_df[col], validation_df[col_ref])\n",
    "                del validation_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_models_dataset = validation_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "homology_models_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `homology_models_dataset_filtered`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots()\n",
    "homology_models_dataset[\"identity_calc\"].hist(bins=100, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTITY_CUTOFF = 1.00\n",
    "\n",
    "query_ids_w3plus = {\n",
    "    query_id\n",
    "    for query_id, group in \n",
    "        homology_models_dataset[\n",
    "            (homology_models_dataset[\"identity_calc\"] <= IDENTITY_CUTOFF)\n",
    "        ]\n",
    "        .groupby('query_id')\n",
    "    if len(group) >= 10\n",
    "}\n",
    "\n",
    "homology_models_dataset_filtered = (\n",
    "    homology_models_dataset[\n",
    "        (homology_models_dataset[\"identity_calc\"] <= IDENTITY_CUTOFF) &\n",
    "        (homology_models_dataset['query_id'].isin(query_ids_w3plus))\n",
    "    ]\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "print(len(homology_models_dataset))\n",
    "print(len(homology_models_dataset_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `homology_models_dataset_final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homology_models_dataset_final = homology_models_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_models_dataset_final = homology_models_dataset_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'dope_score',\n",
    "    'dope_score_norm',\n",
    "    'ga341_score',\n",
    "    'rosetta_score',\n",
    "]\n",
    "\n",
    "feature_columns = [\n",
    "    \"identity_calc\",\n",
    "    # \"coverage_calc\", \n",
    "\n",
    "    \"identity\", \n",
    "    \"similarity\",\n",
    "    \"score\",  # \"probability\", \"evalue\",\n",
    "    \"sum_probs\",\n",
    "    \n",
    "#     \"query_match_length\", \n",
    "#     \"template_match_length\",\n",
    "]\n",
    "\n",
    "network_columns = [\n",
    "    c\n",
    "    for c in homology_models_dataset_final.columns\n",
    "    if (c.endswith(\"_pdb\") or c.endswith(\"_hm\"))\n",
    "    and not (c.startswith(\"adjacency_idx\") or c.startswith(\"frac_aa_wadj\"))\n",
    "]\n",
    "\n",
    "results_df = homology_models_dataset_final.dropna(subset=network_columns).copy()\n",
    "print(f\"Lost {len(homology_models_dataset_final) - len(results_df)} columns with nulls!\")\n",
    "\n",
    "for col in ['dope_score', 'dope_score_norm', 'rosetta_score']:\n",
    "    results_df[col] = -results_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(network_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations for each sequence independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for query_id, group in results_df.groupby('query_id'):\n",
    "    assert (group['sequence'].str.replace('-', '') == group['sequence'].iloc[0].replace('-', '')).all()\n",
    "    assert (group['query_match_length'] == group['query_match_length'].iloc[0]).all()\n",
    "\n",
    "    if len(group) < 3:\n",
    "        print(f\"Skipping small group for query_id = '{query_id}'\")\n",
    "        continue\n",
    "\n",
    "    for y_col in target_columns:\n",
    "        if len(group) < 3 or len(set(group[y_col])) == 1:\n",
    "            print(f\"skipping y_col '{y_col}'\")\n",
    "            continue\n",
    "        for x_col in feature_columns + network_columns:\n",
    "            if x_col in ['query_match_length']:\n",
    "                continue\n",
    "            if len(group) < 3 or len(set(group[x_col])) == 1:\n",
    "                print(f\"skipping x_col '{x_col}'\")\n",
    "                continue\n",
    "            corr, pvalue = stats.spearmanr(group[x_col], group[y_col])\n",
    "            data.append((y_col, x_col, corr, pvalue))\n",
    "            \n",
    "correlations_df = pd.DataFrame(data, columns=['target', 'feature', 'correlation', 'pvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_columns_sorted = (\n",
    "    correlations_df[\n",
    "        (correlations_df['target'] == 'dope_score_norm') &\n",
    "        (correlations_df['feature'].isin(network_columns))\n",
    "    ]\n",
    "    .groupby(\"feature\", as_index=True)\n",
    "    ['correlation']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "assert len(network_columns_sorted) == len(network_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, columns):\n",
    "    \n",
    "    mat = np.zeros((len(columns), len(columns)), float)\n",
    "    for i, c1 in enumerate(columns):\n",
    "        for j, c2 in enumerate(columns):\n",
    "            mat[i, j] = stats.spearmanr(df[c1], df[c2])[0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(mat)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(columns)))\n",
    "    ax.set_yticks(np.arange(len(columns)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(columns)\n",
    "    ax.set_yticklabels(columns)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(len(columns)):\n",
    "            text = ax.text(j, i, f\"{mat[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Spearman correlation between alignment, structure, and network scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = target_columns + feature_columns + network_columns_sorted\n",
    "dim = 4 + 0.4 * len(features)\n",
    "\n",
    "with plt.rc_context(rc={'figure.figsize': (dim, dim), 'font.size': 11}):\n",
    "    plot(results_df, features)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH.joinpath(\"validation_homology_models_corr_all.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(OUTPUT_PATH.joinpath(\"validation_homology_models_corr_all.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ignore = ['query_match_length']\n",
    "features = [c for c in feature_columns + network_columns_sorted if c not in ignore]\n",
    "figsize = (2 + 0.5 * len(features), 6)\n",
    "\n",
    "for i, target in enumerate(target_columns):\n",
    "    corr = [\n",
    "        correlations_df[\n",
    "            (correlations_df['target'] == target) &\n",
    "            (correlations_df['feature'] == feature)\n",
    "        ]['correlation'].values\n",
    "        for feature in features\n",
    "    ]\n",
    "    with plt.rc_context(rc={'figure.figsize': figsize, 'font.size': 14}):\n",
    "        plt.boxplot(corr)\n",
    "        plt.ylim(-0.55, 1.05)\n",
    "        plt.xticks(range(1, len(features) + 1), features, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        plt.ylabel(\"Spearman R\")\n",
    "        plt.title(f\"{target} (identity cutoff: {IDENTITY_CUTOFF:.2})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_PATH.joinpath(f\"{target}_corr_gby_query.png\"), dpi=300, bbox_inches=\"tight\", transparent=False, frameon=True)\n",
    "        plt.savefig(OUTPUT_PATH.joinpath(f\"{target}_corr_gby_query.pdf\"), bbox_inches=\"tight\", transparent=False, frameon=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "713px",
    "left": "23px",
    "top": "144px",
    "width": "346px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
