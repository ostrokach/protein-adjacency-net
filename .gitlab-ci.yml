image: registry.gitlab.com/ostrokach-docker/conda:latest

stages:
  - init
  - notebooks-01
  - notebooks-03
  - notebooks-04
  - notebooks-05
  - deploy

# === Variables ===

variables:
  PROJECT_VERSION: "0.2"
  DOCS_SECRET_KEY: "SWKhaBJiG8O53MTcw97h1vRjVgimQCb4mzTIH4H8dD10vrb6NWOUantGWhqHMoY7"
  CONDA_ENVIRONMENT_NAME: ci-${CI_PROJECT_PATH_SLUG}-${CI_COMMIT_REF_SLUG}

.niagara-envs: &niagara-envs # Uses all memory on node by default
  SBATCH_ARGS: "--account=def-pmkim"
  # SBATCH_EXTRA_ARGS: "--time 12:00:00 --array=1-1"

.graham-envs: &graham-envs
  SBATCH_ARGS: "--account=def-pmkim --exclusive --mem=0"
  # SBATCH_EXTRA_ARGS: "--time 12:00:00 --array=1-1"

.orca-envs: &orca-envs
  SBATCH_ARGS: "--account=rrg-pmkim --exclusive --mem=0"
  # SBATCH_EXTRA_ARGS: "--time 12:00:00 --array=1-1"

.cedar-envs: &cedar-envs
  SBATCH_ARGS: "--account=rrg-pmkim --exclusive --mem=0 --constraint=skylake"
  # SBATCH_EXTRA_ARGS: "--time 12:00:00 --array=1-1"

.cluster-tags: &cluster-tags
  tags:
    - niagara-ssh

.cluster-envs: &cluster-envs
  SBATCH_SCRIPT: "./scripts/qsub.sh"
  <<: *niagara-envs

# === Basic GitLab runner tests ===

.test: &test
  stage: init
  script:
    - echo "starting test..."
    - find .
    - sleep 1
    - mkdir -p some-folder
    - echo "hello world" > some-folder/greeting.txt
    - echo "$(date --iso-8601=seconds)" > some-folder/time.txt
    - echo "done"
  artifacts:
    paths:
      - some-folder
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*tests(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*tests(,\s?.*)*\]/i

test-niagara-ssh:
  <<: *test
  tags:
    - niagara-ssh

test-graham-ssh:
  <<: *test
  tags:
    - graham-ssh

test-cedar-ssh:
  <<: *test
  tags:
    - cedar-ssh

# === Conda environment ===

create_conda_environment:
  stage: init
  <<: *cluster-tags
  script:
    - conda env update -q -n ${CONDA_ENVIRONMENT_NAME} -f environment-lock.yaml
    - source activate ${CONDA_ENVIRONMENT_NAME}
    - pip install -U git+https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.com/tkpod/tkpod-core.git@v0.1.6
    - pip install -U git+https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.com/tkpod/tkpod-modeller.git@v0.1.5
    - pip install -U git+https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.com/kimlab/pagnn.git@f5545393f82d098dd8652a65baec467b62e3d696
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*create_conda_environment(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*create_conda_environment(,\s?.*)*\]/i
    changes:
      # Not working together with variables ATM
      - environment-lock.yaml

# === Batch jobs ===

.sbatch-setup: &sbatch-setup
  before_script:
    # Make sure all required environment variables have been set
    - REQUIRED_VARS=(
      CONDA_ENVIRONMENT_NAME
      DATAPKG_INPUT_DIR
      DATAPKG_OUTPUT_DIR
      CI_PROJECT_NAME
      CI_COMMIT_REF_SLUG
      CI_JOB_NAME
      CI_JOB_ID
      )
    - for var in ${REQUIRED_VARS[*]} ; do
      if [[ -z "${!var}" ]] ; then
      echo "Environment variable '${var}' has not been set!" ;
      exit -1 ;
      fi ;
      done
    # Activate conda environment
    - source activate ${CONDA_ENVIRONMENT_NAME}
    # Set output directory
    - export OUTPUT_DIR=${DATAPKG_OUTPUT_DIR}/${CI_PROJECT_NAME}/${CI_COMMIT_SHA}/${CI_JOB_NAME}
    - export OUTPUT_DIR_FINAL=${DATAPKG_OUTPUT_DIR}/${CI_PROJECT_NAME}/v${PROJECT_VERSION}/${CI_JOB_NAME}/${CI_COMMIT_SHA}
    - echo "export OUTPUT_DIR=${OUTPUT_DIR}" | tee -a env.sh
    - echo "export OUTPUT_DIR_FINAL=${OUTPUT_DIR_FINAL}" | tee -a env.sh
    - if [[ -d ${OUTPUT_DIR} || -d ${OUTPUT_DIR_FINAL} && -z ${ORIGINAL_ARRAY_TASK_COUNT} ]] ; then
      echo "Either OUTPUT_DIR or OUTPUT_DIR_FINAL already exist!" ;
      exit -1 ;
      fi
    - mkdir -p "${OUTPUT_DIR}" "${OUTPUT_DIR}/logs" "${OUTPUT_DIR}/notebooks"

.sbatch: &sbatch
  script:
    - sbatch
      --nodes=1
      --ntasks-per-node=1
      --job-name=${CI_JOB_NAME}
      --mail-user=alexey.strokach@kimlab.org
      --mail-type=ALL
      --chdir=`pwd`
      --output=${OUTPUT_DIR}/logs/sbatch-${CI_JOB_ID}-%N-%A-%a.log
      --export=ALL
      --wait
      ${SBATCH_ARGS}
      ${SBATCH_EXTRA_ARGS}
      ${SBATCH_SCRIPT} |
      tee sbatch.out

.sbatch-teardown: &sbatch-teardown
  after_script:
    # Activate conda environment
    - source activate ${CONDA_ENVIRONMENT_NAME}
    # Set JOB_ID
    - JOB_ID=$(rg "Submitted batch job" sbatch.out | rg -o '\d+$')
    - echo "export JOB_ID=${JOB_ID}" >> env.sh
    # Load environment variables from previous stages
    - source env.sh
    - cp env.sh ${OUTPUT_DIR}/env-${CI_COMMIT_SHA}.sh
    # Save job output to GitLab CI
    - mkdir -p output
    - rsync -a
      --link-dest=${OUTPUT_DIR}
      --include="*.pdf"
      --include="*.png"
      --include="*.svg"
      --include="notebooks/***"
      --include="logs/***"
      --exclude="*"
      ${OUTPUT_DIR}/
      output
    # Update job output to ${OUTPUT_DIR_FINAL}
    - mkdir -p ${OUTPUT_DIR_FINAL}
    - rsync -a
      --link-dest=${OUTPUT_DIR}
      ${OUTPUT_DIR}/
      ${OUTPUT_DIR_FINAL}
    # Display job output
    - for file in output/logs/*.log ; do
      echo -e "\e[1m\e[35m${file}\e[0m" ;
      cat $file ;
      done
    # Display job resource consumption
    - sacct -j ${JOB_ID} --fields JobID,User,Account,AllocNodes,Start,End,Elapsed,AllocTRES,CPUTime,NodeList,ExitCode,State,SystemCPU,UserCPU,TotalCPU,AveDiskRead,MaxDiskRead,AveRSS,MaxRSS,AveVMSize,MaxVMSize -p --delimiter \| | xsv table -d '|'
  artifacts:
    paths:
      - env.sh
      - output
    when: always

.sbatch-output: &sbatch-output
  script:
    - cat env.sh
    - source env.sh
    - ./scripts/wait-for-job-to-finish.sh
    - mkdir -p output
    - cp -al ${OUTPUT_DIR} output/$(basename ${OUTPUT_DIR})
    - (cd output; ln -s )
  artifacts:
    paths:
      - output

# === User code ===

.notebook:
  <<: [*sbatch-setup, *sbatch, *sbatch-teardown]
  <<: *cluster-tags
  variables:
    <<: *cluster-envs

decoy_discrimination_dataset:
  extends: .notebook
  stage: notebooks-01
  variables:
    NOTEBOOK_NAME: "01-decoy_discrimination_dataset"
    SBATCH_EXTRA_ARGS: "--array=1-200 --time 3:00:00 "
    # ORIGINAL_ARRAY_TASK_COUNT: "200"
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*decoy_discrimination_dataset(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*decoy_discrimination_dataset(,\s?.*)*\]/i

decoy_discrimination_dataset_rosetta:
  extends: .notebook
  stage: notebooks-01
  variables:
    NOTEBOOK_NAME: "01-decoy_discrimination_dataset"
    SBATCH_EXTRA_ARGS: "--array=1-200 --time 24:00:00 "
    # ORIGINAL_ARRAY_TASK_COUNT: "200"
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*decoy_discrimination_dataset_rosetta(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*decoy_discrimination_dataset_rosetta(,\s?.*)*\]/i

# add_adjacency_distances

add_adjacency_distances-training_dataset:
  extends: .notebook
  stage: notebooks-03
  variables:
    NOTEBOOK_NAME: "03-add_adjacency_distances"
    # SBATCH_EXTRA_ARGS: "--array=15-100 --time 24:00:00 "
    SBATCH_EXTRA_ARGS: "--array=28,30,42,51,53,81,82,84,88 --time 72:00:00 "
    SBATCH_ARGS: "--account=rrg-pmkim --exclusive --mem 257000M"
    ORIGINAL_ARRAY_TASK_COUNT: "100"
    ADJACENCY_MATRIX_PARQUET_PATH: "~/datapkg_output_dir/adjacency-net-v2/master/training_dataset/adjacency_matrix.parquet"
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*add_adjacency_distances-training_dataset(,\s?.*)*\z/i

add_adjacency_distances-validation_dataset:
  extends: .notebook
  stage: notebooks-03
  variables:
    NOTEBOOK_NAME: "03-add_adjacency_distances"
    SBATCH_EXTRA_ARGS: "--array=4-4 --time 24:00:00 "
    ADJACENCY_MATRIX_PARQUET_PATH: "~/datapkg_output_dir/adjacency-net-v2/master/validation_dataset"
    ORIGINAL_ARRAY_TASK_COUNT: "10"
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*add_adjacency_distances-validation_dataset(,\s?.*)*\z/i

add_adjacency_distances-test_dataset:
  extends: .notebook
  stage: notebooks-03
  variables:
    NOTEBOOK_NAME: "03-add_adjacency_distances"
    SBATCH_EXTRA_ARGS: "--array=1-10 --time 24:00:00 "
    ADJACENCY_MATRIX_PARQUET_PATH: "~/datapkg_output_dir/adjacency-net-v2/master/test_dataset"
    # ORIGINAL_ARRAY_TASK_COUNT: "10"
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*add_adjacency_distances-test_dataset(,\s?.*)*\z/i

add_adjacency_distances-adjacency_matrix_subset_sample:
  extends: .notebook
  stage: notebooks-03
  variables:
    NOTEBOOK_NAME: "03-add_adjacency_distances"
    SBATCH_EXTRA_ARGS: "--array=1-1 --time 24:00:00 "
    ADJACENCY_MATRIX_PARQUET_PATH: "~/datapkg_output_dir/hhsuite-wstructure/master/adjacency_matrix_subset_sample"
    # ORIGINAL_ARRAY_TASK_COUNT: "10"
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*add_adjacency_distances-adjacency_matrix_subset_sample(,\s?.*)*\z/i

# train_network

train_network:
  extends: .notebook
  stage: notebooks-03
  variables:
    SBATCH_SCRIPT: "./scripts/train_network.sh"
    SBATCH_TIMELIMIT: "3:00:00"
    SBATCH_EXTRA_ARGS: "--array=0-15%1 --time ${SBATCH_TIMELIMIT} "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*train_network(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*train_network(,\s?.*)*\]/i

# validate_network

validation_training_stats:
  extends: .notebook
  stage: notebooks-04
  variables:
    NOTEBOOK_NAME: "04-validation_training_stats"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_training_stats(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_training_stats(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network(,\s?.*)*\]/i

validation_protherm_dataset:
  extends: .notebook
  stage: notebooks-04
  variables:
    NOTEBOOK_NAME: "04-validation_protherm_dataset"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_protherm_dataset(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_protherm_dataset(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network(,\s?.*)*\]/i

validation_homology_models:
  extends: .notebook
  stage: notebooks-04
  variables:
    NOTEBOOK_NAME: "04-validation_homology_models"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_homology_models(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_homology_models(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network(,\s?.*)*\]/i

validation_remote_homology_detection:
  extends: .notebook
  stage: notebooks-04
  variables:
    NOTEBOOK_NAME: "04-validation_remote_homology_detection"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_remote_homology_detection(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_remote_homology_detection(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network(,\s?.*)*\]/i

validation_decoy_discrimination:
  extends: .notebook
  stage: notebooks-04
  variables:
    NOTEBOOK_NAME: "04-validation_decoy_discrimination"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_decoy_discrimination(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_decoy_discrimination(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network(,\s?.*)*\]/i

# validate_network_combined

validation_training_stats_combined:
  extends: .notebook
  stage: notebooks-05
  variables:
    NOTEBOOK_NAME: "05-validation_training_stats_combined"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_training_stats_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_training_stats_combined(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network_combined(,\s?.*)*\]/i

validation_protherm_dataset_combined:
  extends: .notebook
  stage: notebooks-05
  variables:
    NOTEBOOK_NAME: "05-validation_protherm_dataset_combined"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_protherm_dataset_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_protherm_dataset_combined(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network_combined(,\s?.*)*\]/i

validation_homology_models_combined:
  extends: .notebook
  stage: notebooks-05
  variables:
    NOTEBOOK_NAME: "05-validation_homology_models_combined"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_homology_models_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_homology_models_combined(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network_combined(,\s?.*)*\]/i

validation_remote_homology_detection_combined:
  extends: .notebook
  stage: notebooks-05
  variables:
    NOTEBOOK_NAME: "05-validation_remote_homology_detection_combined"
    SBATCH_EXTRA_ARGS: "--time 3:00:00 "
  only:
    variables:
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validation_remote_homology_detection_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validation_remote_homology_detection_combined(,\s?.*)*\]/i
      - $JOBS_TO_RUN =~ /\A(.*,\s?)*validate_network_combined(,\s?.*)*\z/i
      - $CI_COMMIT_MESSAGE =~ /\[ci jobs?:\s*(.*,\s?)*validate_network_combined(,\s?.*)*\]/i

# === Pages ===

pages:
  stage: deploy
  before_script:
    # Install conda packages
    - conda install -y -q
      git pip ipython jupyter ipykernel pypandoc
    # Install pip packages
    - pip install -q
      sphinx recommonmark sphinx_bootstrap_theme
  script:
    # Generate notebooks.csv
    - ./scripts/create_notebook_table.py -i notebooks/ -o docs/notebooks.csv
    # Build pages
    - sphinx-build docs public/$DOCS_SECRET_KEY
    # Convert notebooks
    - mkdir -p public/$DOCS_SECRET_KEY/notebooks
    - ./scripts/convert_notebooks.sh notebooks/ public/$DOCS_SECRET_KEY/notebooks/
  artifacts:
    paths:
      - public
  only:
    - tags
