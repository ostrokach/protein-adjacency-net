{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import itertools\n",
    "import importlib\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from kmtools import py_tools, sequence_tools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numba import njit, prange\n",
    "from scipy import stats\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import pagnn.models.dcn\n",
    "from pagnn.datavargan import dataset_to_datavar\n",
    "from pagnn.models.common import AdjacencyConv, SequenceConv, SequentialMod\n",
    "from pagnn.utils import expand_adjacency_tensor, padding_amount, reshape_internal_dim\n",
    "from pagnn.dataset import dataset_to_gan, row_to_dataset\n",
    "\n",
    "from kmtools import py_tools, sequence_tools\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from pagnn.types import DataRow, DataSetGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = Path.cwd().joinpath('..', 'src').resolve(strict=True)\n",
    "\n",
    "if SRC_PATH.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH.as_posix())\n",
    "\n",
    "import helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = Path('validation_protherm_dataset')\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(os.getenv('OUTPUT_DIR', NOTEBOOK_PATH.name)).resolve()\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = subprocess.run([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], stdout=subprocess.PIPE)\n",
    "GIT_REV = proc.stdout.decode().strip()\n",
    "GIT_REV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "NETWORK_NAME = os.getenv(\"CI_COMMIT_SHA\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "TASK_ID, TASK_COUNT, NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = \"CI\" not in os.environ    \n",
    "DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    NETWORK_NAME = \",\".join([\n",
    "        \"7b4ff1af3ec63a01fa415435420c554be1fecbb0\",  # test74\n",
    "    ])\n",
    "else:\n",
    "    assert NETWORK_NAME is not None\n",
    "    \n",
    "NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DATAPKG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPKG[\"protherm_validaton_dataset\"] = Path(os.environ[\"DATAPKG_OUTPUT_DIR\"]).joinpath(\n",
    "    \"adjacency-net-v2\", \"v0.2\", \"protherm_dataset\", \"protherm_validaton_dataset.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = DATAPKG[\"protherm_validaton_dataset\"].resolve(strict=True)\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pq.read_table(input_file).to_pandas()\n",
    "input_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(input_df['cartesian_ddg_beta_nov16_cart_1'].values, input_df['ddg_exp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load master network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_NETWORKS[NETWORK_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_info = TRAINED_NETWORKS[NETWORK_NAME]['network_info']\n",
    "network_file = TRAINED_NETWORKS[NETWORK_NAME]['network_file']\n",
    "network_state = Path(TRAINED_NETWORKS[NETWORK_NAME]['network_state'])\n",
    "\n",
    "runpy.run_path(network_file)\n",
    "\n",
    "Net = getattr(pagnn.models.dcn, network_info[\"network_name\"])\n",
    "net = Net(**network_info[\"network_settings\"])\n",
    "net.load_state_dict(torch.load(network_state.as_posix()))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TL network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TupleToDataSet:\n",
    "    \n",
    "    def __init__(self, dataset_to_datavar):\n",
    "        self.dataset_to_datavar = dataset_to_datavar\n",
    "        \n",
    "    def __call__(self, tup):\n",
    "        # DataRow\n",
    "        row_pos = DataRow(\n",
    "            sequence=tup.sequence,\n",
    "            adjacency_idx_1=tup.adjacency_idx_1,\n",
    "            adjacency_idx_2=tup.adjacency_idx_2,\n",
    "            distances=tup.distances,\n",
    "            target=0\n",
    "        )\n",
    "        row_neg = DataRow(\n",
    "            sequence=tup.sequence_mut,\n",
    "            adjacency_idx_1=tup.adjacency_idx_1,\n",
    "            adjacency_idx_2=tup.adjacency_idx_2,\n",
    "            distances=tup.distances,\n",
    "            target=tup.ddg_exp,\n",
    "        )\n",
    "\n",
    "        # DataSet\n",
    "        permute_offset = pagnn.dataset.get_offset(len(tup.sequence.replace('-', '')), np.random.RandomState())\n",
    "        dataset_pos = dataset_to_gan(row_to_dataset(row_pos, permute_offset=permute_offset))\n",
    "        dataset_neg = dataset_to_gan(row_to_dataset(row_neg, permute_offset=permute_offset))\n",
    "\n",
    "        assert dataset_pos.adjs == dataset_neg.adjs\n",
    "        dataset = DataSetGAN(\n",
    "            dataset_pos.seqs + dataset_neg.seqs,\n",
    "            dataset_neg.adjs,\n",
    "            dataset_neg.meta,\n",
    "        )\n",
    "        \n",
    "        # DataVar\n",
    "        datavar = self.dataset_to_datavar(dataset)\n",
    "        return datavar, torch.tensor([tup.ddg_exp], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProthermData(Dataset):\n",
    "    \n",
    "    def __init__(self, input_file, transform) -> None:\n",
    "        input_df = pq.read_table(input_file).to_pandas()\n",
    "        input_df['sequence'] = input_df['qseq']\n",
    "        # input_df['sequence_mut'] = input_df.apply(mutate_sequence, axis=1)\n",
    "        input_df['sequence_mut'] = input_df['qseq_mutation']\n",
    "        input_df['adjacency_idx_1'] = input_df['residue_idx_1_corrected']\n",
    "        input_df['adjacency_idx_2'] = input_df['residue_idx_2_corrected']\n",
    "        \n",
    "        columns = [\"sequence\", \"sequence_mut\", \"adjacency_idx_1\", \"adjacency_idx_2\", \"distances\", \"ddg_exp\"]\n",
    "        self.tuples = list(input_df[columns].itertuples())\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tuples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tup = self.tuples[index]\n",
    "        datapoint = self.transform(tup)\n",
    "        return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProthermTransferLearner(nn.Module):\n",
    "    \n",
    "    def __init__(self, master_model) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.master_model = copy.deepcopy(master_model)      \n",
    "        self.master_model.eval()\n",
    "\n",
    "        for param in self.master_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.master_model.layer_n.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "#         for param in self.master_model.layer_n[3].parameters():\n",
    "#             param.requires_grad = True\n",
    "        \n",
    "        input_size = self.master_model.hidden_size\n",
    "        if False:\n",
    "            input_size *= 2  # For wt and mut\n",
    "            hidden_size = input_size * 2\n",
    "        else:\n",
    "            hidden_size = input_size\n",
    "        \n",
    "\n",
    "        self.layer_n = nn.Sequential(\n",
    "#             nn.Conv1d(\n",
    "#                 input_size,\n",
    "#                 hidden_size,\n",
    "#                 kernel_size=self.master_model.kernel_size,\n",
    "#                 stride=self.master_model.stride,\n",
    "#                 padding=self.master_model.padding,\n",
    "#                 bias=True,\n",
    "#             ),\n",
    "            nn.MaxPool1d(4000),\n",
    "            nn.Conv1d(hidden_size, 1, kernel_size=1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, adjs):\n",
    "        x_wt = seq[0:1]\n",
    "        x_wt = self.master_model.layer_1(x_wt, adjs[0][0])\n",
    "\n",
    "        x_mut = seq[1:2]\n",
    "        x_mut = self.master_model.layer_1(x_mut, adjs[0][0])\n",
    "\n",
    "        if False:\n",
    "            x = torch.cat([x_wt, x_mut], dim=1)\n",
    "            x = self.layer_n(x)\n",
    "        elif False:\n",
    "            x_wt = self.master_model.layer_n(x_wt)\n",
    "            x_mut = self.master_model.layer_n(x_mut)\n",
    "            x = (x_wt - x_mut).sum()\n",
    "        else:\n",
    "            x = x_wt - x_mut\n",
    "            x = self.layer_n(x)\n",
    "            x = x.sum()\n",
    "\n",
    "        # Layer N\n",
    "        return x\n",
    "    \n",
    "    def dataset_to_datavar(self, *args, **kwargs):\n",
    "        return self.master_model.dataset_to_datavar(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.decice=(\"cpu\")\n",
    "pagnn.settings.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl = ProthermTransferLearner(net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProthermData(\n",
    "    input_file,\n",
    "    transform=transforms.Compose([\n",
    "        TupleToDataSet(net_tl.dataset_to_datavar),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, target = dataset[0]\n",
    "\n",
    "for dv, target in dataset:\n",
    "    out = net_tl(dv.seqs.to(device), [[dv.adjs[0].to(device)]])\n",
    "    print(out.squeeze(), target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = torch.utils.data.random_split(\n",
    "    dataset, [int(len(dataset) * 0.70), len(dataset) - int(len(dataset) * 0.70)]\n",
    ")\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(dataset_train),\n",
    "    'val': len(dataset_val),\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(dataset_train, batch_size=64, shuffle=False, num_workers=0, collate_fn=list),\n",
    "    \"val\": DataLoader(dataset_train, batch_size=1, shuffle=False, num_workers=0, collate_fn=list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            #             if phase == 'train':\n",
    "            #                 if scheduler is not None:\n",
    "            #                     scheduler.step()\n",
    "            #                 model.train()  # Set model to training mode\n",
    "            #             else:\n",
    "            #                 model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            pred_list = []\n",
    "            target_list = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, batch in enumerate(dataloaders[phase]):\n",
    "                for j, (dv, target) in enumerate(batch):\n",
    "                    dv = dv._replace(seqs=dv.seqs.to(device), adjs=[dv.adjs[0].to(device)])\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        preds = model(dv.seqs, [dv.adjs])\n",
    "                        if False:\n",
    "                            preds_diff = preds[0] - preds[1]\n",
    "                            preds_diff_sum = preds_diff.sum().squeeze()\n",
    "                        else:\n",
    "                            preds_diff_sum = preds.squeeze()\n",
    "\n",
    "                        loss = criterion(preds_diff_sum, target)\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    running_corrects += torch.mean(torch.abs(preds_diff_sum - target.data))\n",
    "                    pred_list.append(preds_diff_sum.cpu().data.numpy())\n",
    "                    target_list.append(target.cpu().data.numpy())\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            pearson_corr = stats.pearsonr(np.hstack(pred_list), np.hstack(target_list))[0]\n",
    "            spearman_corr = stats.spearmanr(np.hstack(pred_list), np.hstack(target_list))[0]\n",
    "\n",
    "            print(\n",
    "                f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} \"\n",
    "                f\"Pearson {pearson_corr:.4f} Spearman {spearman_corr:.4f}\"\n",
    "            )\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training complete in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl = ProthermTransferLearner(net)\n",
    "net_tl = net_tl.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(net_tl.master_model.layer_n.parameters(), lr=0.001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = trains_model(net_tl, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset[4]\n",
    "net_tl(ds.seqs, [ds.adjs]).mean(2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset:\n",
    "    print(net_tl(ds.seqs, [ds.adjs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_neg.adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_pos.adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try transformations like shuffle by same amount\n",
    "dataloader = DataLoader(dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_df[['sequence', 'sequence_mut', 'adjacency_idx_1', 'adjacency_idx_2', 'distances', 'ddg_exp']].copy()\n",
    "\n",
    "from pagnn.dataset import dataset_to_gan, row_to_dataset\n",
    "\n",
    "for row in df.itertuples():\n",
    "    row_pos = DataRow\n",
    "    dataset = dataset_to_gan(row_to_dataset(row, 0))\n",
    "    datavar = net_tl.dataset_to_datavar(dataset)\n",
    "    outputs = net_tl(datavar.seqs, [datavar.adjs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trained_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(row):\n",
    "    sequence = row['sequence']\n",
    "    wt = row['mutation'][0]\n",
    "    pos = int(row['mutation'][1:-1])\n",
    "    mut = row['mutation'][-1]\n",
    "    sequence_mut = sequence[:pos - 1] + mut + sequence[pos:]\n",
    "    assert len(sequence) == len(sequence_mut)\n",
    "    return sequence_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['sequence'] = input_df['qseq']\n",
    "input_df['sequence_mut'] = input_df.apply(mutate_sequence, axis=1)\n",
    "# input_df['sequence_mut'] = input_df['qseq_mutation']\n",
    "input_df['adjacency_idx_1'] = input_df['residue_idx_1_corrected']\n",
    "input_df['adjacency_idx_2'] = input_df['residue_idx_2_corrected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    input_df[f'{network_name}_wt'] = helper.predict_with_network(\n",
    "        input_df[['sequence', 'adjacency_idx_1', 'adjacency_idx_2', 'distances']]\n",
    "            .copy(),\n",
    "        network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "        network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "    )\n",
    "    input_df[f'{network_name}_mut'] = helper.predict_with_network(\n",
    "        input_df[['sequence_mut', 'adjacency_idx_1', 'adjacency_idx_2', 'distances']]\n",
    "            .rename(columns={'sequence_mut': 'sequence'}).copy(),\n",
    "        network_state=TRAINED_NETWORKS[network_name]['network_state'],\n",
    "        network_info=TRAINED_NETWORKS[network_name]['network_info'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name in NETWORK_NAME.split(','):\n",
    "    input_df[f'{network_name}_change'] = (\n",
    "        input_df[f'{network_name}_mut'] -\n",
    "        input_df[f'{network_name}_wt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(input_df, preserve_index=True)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    OUTPUT_PATH.joinpath(\"validation_protherm_dataset.parquet\"),\n",
    "    version='2.0',\n",
    "    flavor='spark',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "424px",
    "left": "26px",
    "top": "141px",
    "width": "187px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
